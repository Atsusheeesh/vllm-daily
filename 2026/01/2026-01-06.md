# vLLM Merged PR Report

**Report Date:** 2026-01-06 PST

**Total Merged PRs:** 51

---

## 1. [[Bugfix][Kernel] fix bias adding in triton kernel implemented fused moe](https://github.com/vllm-project/vllm/pull/31676)


### Base Information

- **PR Number:** #31676
- **Author:** [xuebwang-amd](https://github.com/xuebwang-amd)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-06 23:36:14
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31676/files) (1):**
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`

### Summary

**What changed and why**  
The fix reorders operations in the fused MoE kernel to add bias after dequantization instead of before. This corrects a mathematical error where bias was being added to quantized values, which is incorrect since bias is typically not quantized and should be applied to the dequantized result.

**Technical impact**  
This change ensures numerical correctness for quantized models (FP8/INT8) using bias terms in the MoE layer. It aligns the kernel's computation with the proper quantization formula: `y = s_x * s_w * (Wq - zw) * (xq - zx) + bias`. Models relying on fused MoE with quantization will now produce accurate outputs.

**Potential risks**  
If any downstream code implicitly depended on the previous (incorrect) order—such as expecting bias to be quantized—it could break. The fix also assumes bias is always non-quantized; if a future quantization scheme includes bias, this logic may need revisiting. Lack of a dedicated unit test for the kernel increases reliance on end-to-end tests.

**Key insights**  
Always apply non-quantized operations like bias addition after dequantization in quantized kernels. Consider adding a unit test for the fused MoE kernel to catch similar issues early. Verify that all quantization variants (FP8/INT8) are covered in testing, as the change affects multiple code paths.

---

## 2. [[Bugfix][Hardware][AMD] Consolidate FP8 min/max values helper function](https://github.com/vllm-project/vllm/pull/31106)


### Base Information

- **PR Number:** #31106
- **Author:** [c0de128](https://github.com/c0de128)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-06 22:55:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31106/files) (6):**
  - `tests/kernels/quant_utils.py`
  - `tests/kernels/quantization/test_fp8_min_max_helper.py`
  - `vllm/model_executor/layers/quantization/input_quant_fp8.py`
  - `vllm/model_executor/layers/quantization/utils/fp8_utils.py`
  - `vllm/model_executor/layers/quantization/utils/quant_utils.py`
  - `vllm/utils/deep_gemm.py`

### Summary

**What changed and why**  
A new helper function `get_fp8_min_max()` was added to centralize FP8 min/max value logic, replacing duplicated conditional checks across multiple files. This addresses an accuracy issue on ROCm platforms where PyTorch's default `finfo.max` (240.0) for `torch.float8_e4m3fnuz` is incorrect for dynamic quantization, requiring 224.0 instead.

**Technical impact**  
The change improves code maintainability by eliminating scattered platform-specific logic and ensures consistent FP8 value handling. It standardizes min/max retrieval for FP8 quantization across the codebase, reducing the risk of inconsistencies in quantization behavior, especially on ROCm hardware.

**Potential risks**  
If the helper function’s platform detection logic fails or is misconfigured, it could lead to incorrect quantization bounds and accuracy degradation. The change assumes `current_platform.is_fp8_fnuz()` accurately identifies ROCm MI300 systems; any misalignment in platform detection may propagate errors to all dependent modules.

**Key insights**  
Developers should use `get_fp8_min_max()` for all FP8 min/max needs to ensure consistency. The added unit tests provide a solid foundation for validation, but hardware-specific testing on ROCm is essential. This refactor exemplifies good practice in consolidating platform-dependent constants.

---

## 3. [[BugFix] LoRA: Support loading base_layer of experts](https://github.com/vllm-project/vllm/pull/31104)


### Base Information

- **PR Number:** #31104
- **Author:** [HollowMan6](https://github.com/HollowMan6)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-01-06 22:49:40
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31104/files) (35):**
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/models/afmoe.py`
  - `vllm/model_executor/models/bailing_moe.py`
  - `vllm/model_executor/models/deepseek_eagle.py`
  - `vllm/model_executor/models/deepseek_mtp.py`
  - `vllm/model_executor/models/deepseek_v2.py`
  - `vllm/model_executor/models/dots1.py`
  - `vllm/model_executor/models/ernie45_moe.py`
  - `vllm/model_executor/models/ernie45_vl_moe.py`
  - `vllm/model_executor/models/glm4_moe.py`
  - `vllm/model_executor/models/glm4_moe_mtp.py`
  - `vllm/model_executor/models/gpt_oss.py`
  - `vllm/model_executor/models/granitemoe.py`
  - `vllm/model_executor/models/grok1.py`
  - `vllm/model_executor/models/hunyuan_v1.py`
  - `vllm/model_executor/models/jamba.py`
  - `vllm/model_executor/models/kimi_linear.py`
  - `vllm/model_executor/models/kimi_vl.py`
  - `vllm/model_executor/models/lfm2_moe.py`
  - `vllm/model_executor/models/llama4.py`
  - `vllm/model_executor/models/longcat_flash.py`
  - `vllm/model_executor/models/mimo_v2_flash.py`
  - `vllm/model_executor/models/minimax_m2.py`
  - `vllm/model_executor/models/mixtral.py`
  - `vllm/model_executor/models/mllama4.py`
  - `vllm/model_executor/models/nemotron_h.py`
  - `vllm/model_executor/models/olmoe.py`
  - `vllm/model_executor/models/openpangu.py`
  - `vllm/model_executor/models/openpangu_mtp.py`
  - `vllm/model_executor/models/phimoe.py`
  - `vllm/model_executor/models/qwen2_moe.py`
  - `vllm/model_executor/models/qwen3_moe.py`
  - `vllm/model_executor/models/qwen3_next.py`
  - `vllm/model_executor/models/qwen3_next_mtp.py`
  - `vllm/model_executor/models/transformers/moe.py`

### Summary

**What changed and why**  
This PR fixes a bug in LoRA weight loading for Mixture-of-Experts (MoE) models. The issue was that when LoRA is enabled, the `base_layer` suffix was incorrectly placed in the parameter mapping, causing weight loading to fail. The fix adjusts the `make_expert_params_mapping` method to detect LoRA-enabled models and properly format the parameter names with `base_layer.` inserted at the correct position.

**Technical impact**  
The changes affect weight loading across 34 different MoE model implementations by modifying the core mapping logic in `SharedFusedMoE.make_expert_params_mapping`. All MoE model files now pass the model instance to this method, enabling dynamic detection of LoRA patterns. This ensures compatibility with LoRA fine-tuned checkpoints while maintaining backward compatibility with standard models.

**Potential risks**  
The detection logic relies on checking for `.base_layer.` in parameter names, which could produce false positives if other components use similar naming patterns. Additionally, the widespread changes across many model files increase the risk of missing edge cases in less frequently used models. The fix assumes a consistent LoRA parameter naming convention that may not cover all variations.

**Key insights**  
Developers should verify that the LoRA detection logic works correctly for all supported MoE architectures, especially when loading custom fine-tuned checkpoints. The changes are minimal and focused, but thorough testing across different model types is recommended to ensure no regression in standard (non-LoRA) weight loading scenarios.

---

## 4. [[Bugfix] Fix race condition in async-scheduling for vlm model](https://github.com/vllm-project/vllm/pull/31841)


### Base Information

- **PR Number:** #31841
- **Author:** [tianshu-Michael-yu](https://github.com/tianshu-Michael-yu)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-06 22:45:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31841/files) (1):**
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
The fix addresses a race condition in async scheduling for VLM models by implementing double buffering for the `is_mm_embed` tensor. Previously, with `--async-scheduling` enabled, a previous iteration's asynchronous copy could still be reading from the CPU buffer while the current iteration writes to it, causing sporadic engine crashes. The change replaces the single buffer with two buffers and alternates between them each iteration.

**Technical impact**  
This modification ensures thread-safe access to the `is_mm_embed` buffer during concurrent multimodal request processing. The architecture now uses a ping-pong buffer strategy (`is_mm_embed_buffers` and `is_mm_embed_idx`) to isolate read and write operations across iterations, preventing data corruption and stabilizing the engine under sustained load.

**Potential risks**  
If the buffer index toggling logic (`self.is_mm_embed_idx = 1 - self.is_mm_embed_idx`) is called from multiple threads or contexts incorrectly, it could reintroduce race conditions. Additionally, the fix assumes exactly two buffers are sufficient; extreme concurrency or very high iteration rates might still expose timing issues, though this is unlikely given the described use case.

**Key insights**  
The solution is a classic double-buffering pattern effective for producer-consumer race conditions. Developers should note that this pattern is now critical for `--async-scheduling` with multimodal models and ensure any future modifications to `_gather_mm_embeddings` maintain the buffer-swapping discipline. Consider adding a comment or assertion to guarantee the index toggle occurs only once per iteration.

---

## 5. [refactor: find_loaded_library](https://github.com/vllm-project/vllm/pull/31866)


### Base Information

- **PR Number:** #31866
- **Author:** [tom-zju](https://github.com/tom-zju)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-06 22:42:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31866/files) (3):**
  - `vllm/device_allocator/cumem.py`
  - `vllm/distributed/device_communicators/cuda_wrapper.py`
  - `vllm/utils/system_utils.py`

### Summary

**What changed and why**  
The `find_loaded_library` function was duplicated across two files (`cumem.py` and `cuda_wrapper.py`). This refactor consolidates the function into a single location (`system_utils.py`) to eliminate code duplication and improve maintainability. The implementation from `cumem.py` was chosen as the canonical version due to its correct variable naming (`found_line` vs. `found` and `line`).

**Technical impact**  
This change centralizes library path resolution logic, reducing redundancy and ensuring consistent behavior across the codebase. Both `cumem.py` and `cuda_wrapper.py` now import the shared utility, simplifying future updates and reducing the risk of divergence between implementations.

**Potential risks**  
The refactor introduces a dependency on `system_utils.py` for both modules, which may affect startup if import cycles arise. Additionally, the function assumes a Linux environment (due to `/proc/self/maps`), which could cause failures on non-Linux platforms unless properly guarded.

**Key insights**  
Consolidating duplicate code is a positive step toward cleaner architecture. Developers should verify that all callers handle the `None` return value appropriately. Consider adding platform checks or fallbacks to ensure cross-platform compatibility if the codebase expands beyond Linux.

---

## 6. [[Attention][3/n] Remove usage of deprecated `seq_lens_cpu` and `num_computed_tokens_cpu` CommonAttentionMetadata properties](https://github.com/vllm-project/vllm/pull/31850)


### Base Information

- **PR Number:** #31850
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-06 21:31:34
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31850/files) (2):**
  - `vllm/v1/attention/backends/rocm_aiter_fa.py`
  - `vllm/v1/attention/backends/rocm_attn.py`

### Summary

**What changed and why**  
The changes replace deprecated `seq_lens_cpu` property usage with `seq_lens.cpu()` calls in ROCm attention backends. This aligns the code with an updated CommonAttentionMetadata interface where `seq_lens_cpu` is being phased out in favor of accessing `seq_lens` directly and explicitly moving to CPU when needed.

**Technical impact**  
These modifications ensure compatibility with the evolving attention metadata API while maintaining identical functionality. The changes are minimal and localized to two ROCm-specific backend files, indicating a systematic deprecation cleanup across attention implementations.

**Potential risks**  
If `seq_lens` is not consistently available as a tensor across all execution paths or differs in device placement from the deprecated property, it could introduce subtle bugs. The explicit `.cpu()` calls assume CPU processing is required, which may not hold for all future use cases.

**Key insights**  
Developers should verify that `seq_lens` tensor device placement aligns with backend requirements. This cleanup suggests broader deprecation efforts; similar updates may be needed in other backends. The changes are straightforward but highlight the importance of tracking API evolution in distributed components.

---

## 7. [[ROCm][AITER] bugfix accuracy regression in ROCM_AITER_TRITON_MLA backend](https://github.com/vllm-project/vllm/pull/31816)


### Base Information

- **PR Number:** #31816
- **Author:** [vllmellm](https://github.com/vllmellm)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-06 21:04:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31816/files) (1):**
  - `vllm/v1/attention/backends/mla/aiter_triton_mla.py`

### Summary

**What changed and why**  
The PR fixes an accuracy regression in the ROCM_AITER_TRITON_MLA backend by changing the base class of `AiterTritonMLABackend` from `MLACommonBackend` to `AiterMLABackend`. This ensures the backend inherits the correct metadata builder and implementation logic shared with other Aiter Triton backends, restoring proper attention computation.

**Technical impact**  
This change aligns the Aiter Triton MLA backend with the appropriate abstraction layer, ensuring it uses the same metadata construction and kernel selection as other Aiter backends. It resolves low accuracy in models like DeepSeek-V3 by correcting attention kernel behavior under ROCm.

**Potential risks**  
If `AiterMLABackend` introduces any differences in kernel dispatch or metadata handling compared to `MLACommonBackend`, other models or configurations might be affected. The fix assumes the shared Aiter backend logic is universally suitable for this Triton variant, which should be validated across diverse workloads.

**Key insights**  
Always verify that backend inheritance matches the intended abstraction hierarchy, especially when multiple backends share core logic. The dramatic accuracy improvement (0.0067 → 0.96) highlights the critical impact of correct backend composition. Developers should ensure similar backends consistently derive from the same base class to avoid subtle regressions.

---

## 8. [[Chore] Try remove `init_cached_hf_modules`](https://github.com/vllm-project/vllm/pull/31786)


### Base Information

- **PR Number:** #31786
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-06 20:34:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31786/files) (9):**
  - `tests/model_executor/model_loader/runai_streamer_loader/conftest.py`
  - `tests/model_executor/model_loader/tensorizer_loader/conftest.py`
  - `vllm/utils/import_utils.py`
  - `vllm/v1/executor/multiproc_executor.py`
  - `vllm/v1/executor/ray_executor.py`
  - `vllm/v1/executor/uniproc_executor.py`
  - `vllm/v1/worker/gpu_worker.py`
  - `vllm/v1/worker/tpu_worker.py`
  - `vllm/v1/worker/worker_base.py`

### Summary

**What changed and why**  
This PR removes the `init_cached_hf_modules` function and its usage throughout the codebase. The change simplifies worker wrapper initialization by eliminating the need to pass `vllm_config` during wrapper construction and removing the early initialization of Hugging Face modules when `trust_remote_code` is enabled.

**Technical impact**  
The initialization logic is streamlined by moving `vllm_config` assignment entirely into the `init_worker` method rather than during wrapper construction. This reduces complexity in executor implementations (multiproc, ray, uniproc) and worker classes (GPU/TPU). The removal of early HF module initialization suggests that serialization issues with transformer modules may have been resolved.

**Potential risks**  
If transformer modules still require lazy initialization for proper serialization across processes, removing `init_cached_hf_modules` could cause issues when `trust_remote_code=True`. The changes also modify the `execute_model` method signature in `WorkerWrapperBase`, which could break compatibility with existing extensions or custom implementations.

**Key insights**  
The PR successfully simplifies initialization flow but should be thoroughly tested with models using `trust_remote_code`. Developers should verify that all executor types (multiproc, ray, uniproc) work correctly with the new initialization pattern. The signature change to `execute_model` may require updates in dependent code.

---

## 9. [fixed mypy warnings for files vllm/v1/attention with TEMPORARY workaround](https://github.com/vllm-project/vllm/pull/31465)


### Base Information

- **PR Number:** #31465
- **Author:** [MrIceCreamMan](https://github.com/MrIceCreamMan)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-06 20:08:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31465/files) (18):**
  - `tools/pre_commit/mypy.py`
  - `vllm/attention/backends/abstract.py`
  - `vllm/model_executor/layers/attention_layer_base.py`
  - `vllm/v1/attention/backends/flash_attn.py`
  - `vllm/v1/attention/backends/flash_attn_diffkv.py`
  - `vllm/v1/attention/backends/flashinfer.py`
  - `vllm/v1/attention/backends/flex_attention.py`
  - `vllm/v1/attention/backends/gdn_attn.py`
  - `vllm/v1/attention/backends/mamba2_attn.py`
  - `vllm/v1/attention/backends/mamba_attn.py`
  - `vllm/v1/attention/backends/mla/aiter_triton_mla.py`
  - `vllm/v1/attention/backends/mla/common.py`
  - `vllm/v1/attention/backends/mla/flashattn_mla.py`
  - `vllm/v1/attention/backends/mla/flashmla_sparse.py`
  - `vllm/v1/attention/backends/mla/rocm_aiter_mla.py`
  - `vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse.py`
  - `vllm/v1/attention/backends/tree_attn.py`
  - `vllm/v1/attention/backends/utils.py`

### Summary

**What changed and why**  
This PR enables strict mypy type checking for the `vllm/v1/attention` module by moving it from SEPARATE_GROUPS to FILES in the mypy configuration. The changes fix type errors across multiple attention backend implementations, primarily addressing None handling, type assertions, and inheritance issues. A temporary workaround using `# type: ignore[attr-defined]` is applied for dynamically built C extension imports.

**Technical impact**  
The changes improve type safety across the attention module by enforcing stricter type checking, which will catch type-related bugs earlier in development. Modifications include converting `AttentionType` to an Enum, adding required attributes to `AttentionImpl`, fixing optional parameter handling (especially for `block_size` and `max_cudagraph_size`), and ensuring proper type inheritance for metadata classes. The mypy configuration change now subjects these files to comprehensive type analysis.

**Potential risks**  
The temporary type ignore comments for `vllm_flash_attn` imports could mask legitimate type errors if the CI environment issue isn't resolved. Some changes involve adding assertions (e.g., `assert self.vllm_flash_attn_version is not None`) that could fail at runtime if preconditions aren't met. The refactoring of `block_size` from optional to required in abstract methods may affect implementations that previously relied on it being optional.

**Key insights**  
Developers should prioritize updating the CI workflow to build extensions before type checking to eliminate the temporary workaround. The pattern of converting `sliding_window` to lists when passing to flash attention functions appears consistently across implementations. Pay attention to the new `AttentionImpl` base class requirements (`num_heads`, `head_size`, `scale`) when implementing new backends. The changes demonstrate good practices for handling optional values in performance-critical code.

---

## 10. [Change warning in get_current_vllm_config to report caller's line number](https://github.com/vllm-project/vllm/pull/31855)


### Base Information

- **PR Number:** #31855
- **Author:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-06 19:48:14
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31855/files) (1):**
  - `vllm/config/vllm.py`

### Summary

**What changed and why**  
The change adds `stacklevel=2` parameter to a warning log in `get_current_vllm_config()`. This ensures the warning points to the caller's line number instead of the logging function itself, making it easier to identify the source of invalid calls.

**Technical impact**  
This is a minor debugging enhancement that improves developer experience by providing more accurate stack trace information. It doesn't affect runtime behavior or functionality, only how warning messages are displayed during development and debugging.

**Potential risks**  
If the function is called from nested wrappers or decorators, stacklevel=2 might still point to an intermediate function rather than the original caller. There's also a minimal performance impact from additional stack frame inspection, though negligible for warning logs.

**Key insights**  
Always consider stacklevel when logging warnings in utility functions to help developers quickly locate issues. For more complex call hierarchies, consider calculating stacklevel dynamically or documenting the expected caller context.

---

## 11. [[Doc] Update release docs](https://github.com/vllm-project/vllm/pull/31799)


### Base Information

- **PR Number:** #31799
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-06 19:27:41
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31799/files) (1):**
  - `RELEASE.md`

### Summary

**What changed and why**  
The PR updates the `RELEASE.md` documentation to reflect current release practices. It replaces outdated information about versioning and cadence with a clearer description of the new regular release schedule (every 2 weeks, incrementing minor versions since v0.12.0), clarifies version semantics, and removes the obsolete 2025 release calendar table.

**Technical impact**  
This is a documentation-only change with no impact on code, builds, or runtime behavior. It improves accuracy and maintainability by aligning the release guide with actual practices, which helps developers and users understand the project's release lifecycle.

**Potential risks**  
The removal of the detailed 2025 calendar table could reduce forward visibility for teams planning around specific release dates, though the new cadence description is more sustainable. There is a minor risk if the documentation still omits specific details about emergency patch releases or post-release procedures that were implied in the old "post1" terminology.

**Key insights**  
The updated documentation is more concise and better reflects the project's mature release process. Developers should note the shift to regular minor-version releases and the clarified cherry-pick criteria. Ensure any automated tools or external documentation referencing the old versioning scheme are updated accordingly.

---

## 12. [[Model] Enable LoRA support for PaliGemma](https://github.com/vllm-project/vllm/pull/31656)


### Base Information

- **PR Number:** #31656
- **Author:** [A1c0r-Z](https://github.com/A1c0r-Z)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-01-06 18:09:32
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31656/files) (2):**
  - `docs/models/supported_models.md`
  - `vllm/model_executor/models/paligemma.py`

### Summary

**What changed and why**  
The changes enable LoRA (Low-Rank Adaptation) support for PaliGemma models by adding the `SupportsLoRA` interface and implementing required methods. This allows fine-tuning the vision tower and connector components via LoRA adapters, addressing feature request #31479.

**Technical impact**  
The model now implements the `get_mm_mapping()`, `get_num_mm_encoder_tokens()`, and `get_num_mm_connector_tokens()` methods, which provide identity mappings since PaliGemma uses a simple linear projector. This integrates PaliGemma into vLLM's LoRA infrastructure without altering its core architecture.

**Potential risks**  
The identity mapping assumes the vision token count remains unchanged through the connector; any future architectural changes to the projector could break this assumption. Additionally, the documentation update indicates LoRA support is now available, but thorough testing across different PaliGemma variants is needed to ensure compatibility.

**Key insights**  
Developers can now apply LoRA adapters to PaliGemma's vision components for efficient fine-tuning. Ensure that any custom modifications to the multi-modal projector are reviewed, as the token count helpers assume a linear projection. The changes are minimal and focused, maintaining backward compatibility while extending functionality.

---

## 13. [[1/2][lmcache connector] clean up lmcache multi-process adapter](https://github.com/vllm-project/vllm/pull/31838)


### Base Information

- **PR Number:** #31838
- **Author:** [ApostaC](https://github.com/ApostaC)
- **Merged By:** [KuntaiDu](https://github.com/KuntaiDu)
- **Merged time:** 2026-01-06 18:02:43
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31838/files) (2):**
  - `vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/multi_process_adapter.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector.py`

### Summary

**What changed and why**  
This PR deprecates the current LMCache multi-process adapter within vLLM and prepares for its migration to the LMCache repository. Changes include adding deprecation warnings to adapter classes, renaming a private method to public (`_cleanup_lookup_result` → `cleanup_lookup_result`), and modifying the connector to conditionally import from LMCache if available, falling back to the vLLM version otherwise.

**Technical impact**  
The changes decouple the multi-process adapter from vLLM's codebase, shifting ownership and CI/CD responsibility to LMCache. This reduces vLLM's maintenance burden and ensures the adapter is tested under LMCache's pipeline. The fallback import mechanism maintains backward compatibility during the transition.

**Potential risks**  
If LMCache's new adapter version introduces breaking changes or is not properly released, the fallback may fail, causing import errors. The renamed method (`cleanup_lookup_result`) must be consistently updated across all usages to avoid runtime failures. Deprecation warnings could be missed in logs, leading to prolonged use of outdated code.

**Key insights**  
Developers should update LMCache to the latest version to avoid deprecated imports. Ensure all references to `_cleanup_lookup_result` are updated to `cleanup_lookup_result`. Monitor for import errors after LMCache releases its version, and plan to remove the fallback logic once the migration is complete.

---

## 14. [[Misc][BE] Type coverage for vllm/compilation [1/3]](https://github.com/vllm-project/vllm/pull/31554)


### Base Information

- **PR Number:** #31554
- **Author:** [Lucaskabela](https://github.com/Lucaskabela)
- **Merged By:** [zou3519](https://github.com/zou3519)
- **Merged time:** 2026-01-06 17:37:52
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31554/files) (12):**
  - `vllm/compilation/backends.py`
  - `vllm/compilation/collective_fusion.py`
  - `vllm/compilation/compiler_interface.py`
  - `vllm/compilation/counter.py`
  - `vllm/compilation/cuda_graph.py`
  - `vllm/compilation/fx_utils.py`
  - `vllm/compilation/inductor_pass.py`
  - `vllm/compilation/monitor.py`
  - `vllm/compilation/partition_rules.py`
  - `vllm/compilation/sequence_parallelism.py`
  - `vllm/compilation/torch25_custom_graph_pass.py`
  - `vllm/compilation/vllm_inductor_pass.py`

### Summary

**What changed and why**  
This PR adds type hint coverage to the `vllm/compilation` module to improve maintainability, readability, and reduce silent errors. The changes primarily involve adding explicit return type annotations (e.g., `-> None`, `-> Generator[...]`) and parameter type hints across 12 files, along with minor adjustments to ensure type consistency.

**Technical impact**  
The type hints enhance static analysis with tools like `mypy`, enabling earlier detection of type-related bugs and improving developer experience through better IDE support. The changes are largely additive and non-breaking, as they do not alter runtime behavior but provide clearer contracts for functions and methods.

**Potential risks**  
Some changes involve broadening parameter types (e.g., `device: str` to `device: str \| None`), which could affect downstream code if it relies on strict string types. Additionally, the `-> Any` annotations in contexts like `contextlib.AbstractContextManager[Any]` may be overly permissive, potentially masking type errors.

**Key insights**  
Developers should run `mypy vllm/compilation` to verify type consistency and consider refining broad `Any` types where possible. The PR sets a foundation for stricter type checking, but follow-up work may be needed to address more specific type constraints and ensure full coverage.

---

## 15. [[Frontend] Implement robust video frame recovery for corrupted videos](https://github.com/vllm-project/vllm/pull/29197)


### Base Information

- **PR Number:** #29197
- **Author:** [vSeamar](https://github.com/vSeamar)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-06 17:13:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29197/files) (4):**
  - `docs/features/multimodal_inputs.md`
  - `tests/multimodal/test_video.py`
  - `vllm/benchmarks/datasets.py`
  - `vllm/multimodal/video.py`

### Summary

**What changed and why**  
This PR adds optional video frame recovery functionality to handle corrupted or truncated video files. The system uses a dynamic window forward-scan algorithm that substitutes failed frames with the next successfully grabbed frame within a recovery window (up to the next target frame). This feature is disabled by default (`frame_recovery=False`) and must be explicitly enabled via CLI.

**Technical impact**  
The changes introduce a new recovery path in both `opencv` and `opencv_dynamic` video backends. The algorithm maintains temporal safety by only using frames from after the failed position, preventing temporal inversion. Metadata now includes recovery mappings, and logging provides visibility into recovery attempts. The base behavior remains unchanged when recovery is disabled, ensuring backward compatibility.

**Potential risks**  
Recovery may introduce subtle artifacts if substituted frames differ significantly from the intended frames, potentially affecting model performance. The algorithm assumes forward-scan continuity; severe corruption spanning multiple consecutive frames could lead to incomplete recovery. There is also a minor performance overhead when recovery is enabled due to additional checks and logging.

**Key insights**  
This is a well-designed opt-in feature that enhances robustness without impacting existing workflows. Developers should enable recovery only when processing unreliable video sources. The comprehensive test suite validates both simulated and real corruption scenarios across backends. Ensure proper monitoring of recovery logs in production to identify videos with frequent failures.

---

## 16. [[ROCm][CI] Fix plugin tests (2 GPUs) failures on ROCm and removing `VLLM_FLOAT32_MATMUL_PRECISION` from all ROCm tests](https://github.com/vllm-project/vllm/pull/31829)


### Base Information

- **PR Number:** #31829
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-06 17:12:24
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31829/files) (2):**
  - `.buildkite/test-amd.yaml`
  - `requirements/rocm-test.txt`

### Summary

**What changed and why**  
Removed the `VLLM_FLOAT32_MATMUL_PRECISION="tf32"` environment variable from three ROCm test groups (vision embeddings, Prithvi MAE pooling, and IO processor plugins) that previously required it to avoid precision conflicts with terratorch. Additionally, pinned `albumentations==1.4.6` in the ROCm test requirements to resolve compatibility issues with tensorV2 functions during plugin tests.

**Technical impact**  
These changes simplify the test configuration by eliminating a temporary workaround for precision issues, indicating that the underlying ROCm/torch compatibility problem has been resolved. Pinning the albumentations version ensures consistent test behavior by preventing auto-upgrades to incompatible newer versions.

**Potential risks**  
If the precision conflict with terratorch on ROCm isn't fully resolved in the current torch version, removing the TF32 setting could reintroduce test failures or numerical instability. The albumentations pin may eventually become outdated, potentially causing dependency conflicts with other packages that require newer versions.

**Key insights**  
The removal of the TF32 workaround suggests progress in ROCm ecosystem stability, but developers should monitor test results for any re-emerging precision issues. Dependency pinning should be periodically reviewed to balance stability with security updates. Consider adding a comment explaining why albumentations is pinned for future maintainers.

---

## 17. [[CI] Add warmup run in test_fusion_attn](https://github.com/vllm-project/vllm/pull/31183)


### Base Information

- **PR Number:** #31183
- **Author:** [angelayi](https://github.com/angelayi)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-06 16:31:53
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31183/files) (1):**
  - `tests/compile/test_fusion_attn.py`

### Summary

**What changed and why**  
The PR adds warmup runs before compiling models in `test_attention_quant_pattern` to address intermittent CI failures. The issue occurred when running multiple tests sequentially, where the second test would fail due to potential Triton caching or initialization problems. The fix includes an initial forward pass on both unfused and fused models before compilation, and disables compilation caching via environment variable.

**Technical impact**  
These changes ensure consistent test execution by forcing Triton kernels to initialize properly before timed measurements. The warmup runs mitigate race conditions or uninitialized states in GPU kernels, while disabling the compile cache (`VLLM_DISABLE_COMPILE_CACHE=1`) prevents cached compilation artifacts from affecting test reliability. The test structure now explicitly separates model compilation from execution.

**Potential risks**  
The warmup runs increase test execution time slightly. Disabling compilation caching globally in the test may hide issues that only appear with caching enabled. There is also a risk that the underlying root cause (possibly in Triton) remains unaddressed, potentially surfacing in other contexts.

**Key insights**  
Always consider kernel initialization and caching effects when debugging GPU-related test flakiness. The fix is a workaround; further investigation into Triton's behavior with varying head dimensions is recommended. Ensure warmup runs are minimal and only applied where necessary to avoid unnecessary test slowdowns.

---

## 18. [[Bugfix] Handle mistral tokenizer in get_hf_processor](https://github.com/vllm-project/vllm/pull/31817)


### Base Information

- **PR Number:** #31817
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-06 15:46:56
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31817/files) (1):**
  - `vllm/multimodal/processing.py`

### Summary

**What changed and why**  
The fix addresses an issue where the Mistral tokenizer wrapper wasn't being properly handled in the `get_hf_processor` method. When a `MistralTokenizer` instance is detected, it extracts the underlying Hugging Face transformers tokenizer before passing it to `cached_processor_from_config`.

**Technical impact**  
This change ensures compatibility between vLLM's custom tokenizer wrappers and Hugging Face processor initialization. The `cached_processor_from_config` function likely expects a standard Hugging Face tokenizer interface, which the wrapper doesn't fully provide.

**Potential risks**  
The fix assumes `MistralTokenizer.transformers_tokenizer` always exists and has the correct interface. If other custom tokenizer types emerge, they may need similar handling. There's also a risk of breaking changes if the `MistralTokenizer` interface changes.

**Key insights**  
Always verify custom wrapper compatibility with downstream dependencies. Consider creating a more generic solution (like a `get_transformers_tokenizer()` method) if multiple tokenizer types need similar unwrapping. The import statement inside the method suggests this is a hotfix; it should be moved to the module level for better code organization.

---

## 19. [[ROCm][CI] Pinning timm lib version to fix ImportError in Multi-Modal Tests (Nemotron)](https://github.com/vllm-project/vllm/pull/31835)


### Base Information

- **PR Number:** #31835
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-06 15:23:11
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31835/files) (1):**
  - `requirements/rocm-test.txt`

### Summary

**What changed and why**  
The change pins the `timm` library to version 1.0.17 in the ROCm test requirements file. This addresses an ImportError in multi-modal model tests where a newer version (1.0.23) caused a missing import of `RotaryEmbedding` from `timm.layers.attention_pool2d`.

**Technical impact**  
This ensures consistent dependency resolution for ROCm CI tests, preventing version drift that breaks multi-modal model imports. It aligns the pinned version with what's already specified in `test.txt`, maintaining compatibility across test environments.

**Potential risks**  
Pinning to an older version may delay receiving bug fixes or performance improvements from newer `timm` releases. There's also a risk of dependency conflicts if other packages require a different `timm` version, though this is mitigated by consistency with `test.txt`.

**Key insights**  
Always pin critical dependencies in test environments to ensure reproducible builds. Consider implementing a broader dependency version management strategy to handle transitive dependencies that may introduce breaking changes. Monitor for when it's safe to update `timm` to a newer compatible version.

---

## 20. [[ROCm][CI] Fix ModernBERT token classification test numerical accuracy on ROCm](https://github.com/vllm-project/vllm/pull/31820)


### Base Information

- **PR Number:** #31820
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-06 15:21:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31820/files) (1):**
  - `tests/models/language/pooling/conftest.py`

### Summary

**What changed and why**  
A new pytest configuration file was added to disable ROCm's Flash and Memory-Efficient Scaled Dot-Product Attention (SDP) kernels and set a high precision for float32 matrix multiplication. This addresses intermittent test failures for ModernBERT token classification caused by numerical differences between vLLM's custom kernels and HuggingFace's eager attention implementation on the ROCm platform.

**Technical impact**  
This change globally configures PyTorch's backend settings for the entire test session when running on ROCm. It forces the use of the slower but more numerically stable `math_sdp` kernel and increases matmul precision, which should make test results more consistent and reproducible by aligning the numerical output of HuggingFace's reference implementation with vLLM's kernels.

**Potential risks**  
The configuration applies to all tests in the session, not just the failing ModernBERT test, which could inadvertently mask precision-related issues in other models or tests. Relying on a global session hook introduces a hidden dependency that may be overlooked when adding new tests or platforms. The warning is emitted only once per session, which could be missed in extensive test logs.

**Key insights**  
This is a pragmatic, platform-specific workaround for a known upstream issue in ROCm's SDP kernels. Developers should treat this as a temporary fix and monitor for updates from the ROCm/HuggingFace side to remove it. The `TODO` comment clearly signals the intent to revert this change. When writing or reviewing tests for ROCm, be aware that the precision environment has been artificially constrained, which may affect the detection of legitimate numerical regressions.

---

## 21. [[Spec Decode][UX] Add acceptance stats to `vllm bench serve` report](https://github.com/vllm-project/vllm/pull/31739)


### Base Information

- **PR Number:** #31739
- **Author:** [MatthewBonanni](https://github.com/MatthewBonanni)
- **Merged By:** [benchislett](https://github.com/benchislett)
- **Merged time:** 2026-01-06 13:21:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31739/files) (1):**
  - `vllm/benchmarks/serve.py`

### Summary

**What changed and why**  
This PR adds speculative decoding acceptance statistics to the `vllm bench serve` benchmark report. It introduces a new function to fetch Prometheus metrics from the server, calculates acceptance rates and per-position acceptance statistics during the benchmark, and displays them in the output when speculative decoding is enabled.

**Technical impact**  
The changes extend the benchmarking capabilities by integrating with the server's Prometheus metrics endpoint (`/metrics`) to extract speculative decoding performance data. This adds a new `Speculative Decoding` section to the report, providing insights into draft efficiency, acceptance rates, and token-level acceptance patterns, which are critical for evaluating speculative decoding effectiveness.

**Potential risks**  
- The metric parsing assumes specific Prometheus label formats (e.g., `position="<num>"`), which could break if the server's metric naming changes.  
- Network issues or timeouts when fetching `/metrics` could cause the speculative decoding section to be omitted silently, potentially masking configuration or connectivity problems.  
- The calculation of `acceptance_length` divides by `delta_drafts`, which could lead to division by zero if no drafts were generated during the benchmark.

**Key insights**  
- This enhancement provides valuable operational metrics for tuning speculative decoding configurations, directly linking benchmark results to draft model performance.  
- Developers should ensure the Prometheus endpoint is accessible and that metric names remain consistent across server versions.  
- Consider adding validation or fallback behavior for edge cases like zero drafts to prevent calculation errors.

---

## 22. [Report error log after vllm bench serve](https://github.com/vllm-project/vllm/pull/31808)


### Base Information

- **PR Number:** #31808
- **Author:** [elvircrn](https://github.com/elvircrn)
- **Merged By:** [yewentao256](https://github.com/yewentao256)
- **Merged time:** 2026-01-06 12:24:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31808/files) (1):**
  - `vllm/benchmarks/serve.py`

### Summary

**What changed and why**  
Added error logging for failed requests in the `vllm bench serve` benchmark. The change prints up to 10 error messages from failed outputs when any failures are detected, addressing cases where certain errors (like connection issues) were previously not logged.

**Technical impact**  
This modification enhances observability by making failed requests visible in benchmark results. It adds minimal runtime overhead (only when failures occur) and doesn't affect successful benchmark execution or metric calculations.

**Potential risks**  
Printing raw error messages could expose sensitive information in certain deployment scenarios. The hard-coded limit of 10 errors might hide patterns in large-scale failures, and the console output format may not integrate well with structured logging systems.

**Key insights**  
This is a straightforward debugging improvement that helps identify infrastructure issues during benchmarking. Consider adding a configurable log level or structured logging format for production use, and ensure error messages don't contain sensitive data like credentials or internal URLs.

---

## 23. [Fix RecursionError in MediaWithBytes unpickling](https://github.com/vllm-project/vllm/pull/31191)


### Base Information

- **PR Number:** #31191
- **Author:** [nrghosh](https://github.com/nrghosh)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-06 12:11:26
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31191/files) (3):**
  - `tests/multimodal/test_image.py`
  - `tools/pre_commit/check_pickle_imports.py`
  - `vllm/multimodal/base.py`

### Summary

**What changed and why**  
The fix addresses a `RecursionError` during unpickling of the `MediaWithBytes` dataclass by adding a guard in `__getattr__` to check if the `media` attribute exists before delegation. This occurs because Python's pickle mechanism can instantiate objects without calling `__init__`, leaving `self.media` uninitialized when `__getattr__` is invoked for internal pickle methods. A regression test validates successful pickle roundtrips.

**Technical impact**  
This change ensures `MediaWithBytes` objects can be safely serialized and deserialized, which is critical for distributed computing frameworks like Ray that rely on pickling. The guard prevents infinite recursion while preserving the attribute delegation behavior once `media` is properly set, maintaining compatibility with existing code that depends on delegated attributes.

**Potential risks**  
If `media` is intentionally set to `None` or deleted after initialization, the guard may incorrectly raise `AttributeError` for valid attribute accesses. Additionally, the fix assumes `__dict__` is the canonical storage for `media`; alternative descriptors or property-based storage could bypass this check. The recursion guard only triggers during unpickling, but any other scenario where `media` is missing could now produce unexpected `AttributeError`s.

**Key insights**  
The root cause highlights a common pitfall when combining `__getattr__` with pickle: always validate required attributes exist before delegation. Consider using `hasattr(self, 'media')` instead of checking `__dict__` for broader compatibility. The regression test is essential to prevent future regressions, especially as pickling is often overlooked in unit tests. Ensure similar patterns elsewhere in the codebase are reviewed for the same issue.

---

## 24. [[Quantization][Refactor] Move CPU GPTQ kernel into MP linear](https://github.com/vllm-project/vllm/pull/31801)


### Base Information

- **PR Number:** #31801
- **Author:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-06 11:10:18
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31801/files) (9):**
  - `tests/quantization/test_cpu_wna16.py`
  - `vllm/config/model.py`
  - `vllm/model_executor/layers/quantization/__init__.py`
  - `vllm/model_executor/layers/quantization/cpu_wna16.py`
  - `vllm/model_executor/layers/quantization/gptq_marlin.py`
  - `vllm/model_executor/layers/quantization/kernels/mixed_precision/__init__.py`
  - `vllm/model_executor/layers/quantization/kernels/mixed_precision/cpu.py`
  - `vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama.py`
  - `vllm/model_executor/layers/quantization/utils/marlin_utils.py`

### Summary

**What changed and why**  
The PR refactors CPU GPTQ quantization by moving its kernel implementation from a standalone configuration (`CPUGPTQConfig` and `CPUGPTQLinearMethod`) into the mixed-precision linear kernel framework (`CPUWNA16LinearKernel`). This consolidates CPU quantization support under a unified architecture, aligning with the broader refactoring goal of #31689.

**Technical impact**  
CPU GPTQ is now handled by the `CPUWNA16LinearKernel` within the mixed-precision kernel system, removing the dedicated `cpu_gptq` quantization method. The kernel supports both GPTQ (without zero points) and AWQ (with zero points, though AWQ is not yet implemented). This change simplifies the quantization codebase by reducing duplication and centralizing CPU-specific logic.

**Potential risks**  
The removal of the `cpu_gptq` quantization method could break backward compatibility for users or scripts that explicitly reference this method. Additionally, the new kernel currently only supports GPTQ (symmetric quantization without zero points); AWQ support is marked as `NotImplementedError`, which may cause issues if AWQ models are attempted on CPU. The kernel also imposes strict alignment requirements (input/output sizes multiples of 32), which could fail for certain model architectures.

**Key insights**  
Developers should update any references from `cpu_gptq` to the appropriate mixed-precision kernel. The PR adds a test model (`Qwen/Qwen1.5-0.5B-Chat-GPTQ-Int4`) to validate GPTQ without `g_idx`. When extending CPU quantization, ensure new kernels adhere to the `MPLinearKernel` interface and properly handle weight transformation (e.g., the `_process_gptq_weights` method).

---

## 25. [[ROCm][CI] Fix tests/compile unit tests](https://github.com/vllm-project/vllm/pull/28895)


### Base Information

- **PR Number:** #28895
- **Author:** [charlifu](https://github.com/charlifu)
- **Merged By:** [ProExpertProg](https://github.com/ProExpertProg)
- **Merged time:** 2026-01-06 10:50:43
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/28895/files) (3):**
  - `tests/compile/fullgraph/test_basic_correctness.py`
  - `tests/compile/fullgraph/test_full_cudagraph.py`
  - `tests/compile/test_noop_elimination.py`

### Summary

**What changed and why**  
These changes fix ROCm-specific test failures in compilation unit tests. The modifications include: conditionally setting the attention backend based on platform, skipping unsupported tests on ROCm, and addressing ROCm-specific memory initialization and in-place operation issues.

**Technical impact**  
The changes improve platform compatibility by dynamically selecting `ATTN_BACKEND` (FLASH_ATTN for CUDA, ROCM_ATTN for ROCm) and skipping tests for unsupported features like encoder self-attention and FlashInfer on ROCm. Additionally, they replace `torch.empty` with `torch.randn` and avoid in-place operations to ensure deterministic behavior on ROCm.

**Potential risks**  
Skipping tests for encoder self-attention on ROCm could mask regressions if the feature is later implemented. The replacement of `torch.empty` with `torch.randn` may slightly alter test behavior due to random initialization, though it ensures memory is initialized. Conditional logic based on `current_platform.is_rocm()` increases maintenance overhead if platform-specific code expands.

**Key insights**  
Developers should ensure platform-specific skips are documented and revisited when ROCm support evolves. The use of `torch.randn` over `torch.empty` is a best practice for ROCm to avoid uninitialized memory issues. In-place operations should be avoided in cross-platform tests to prevent nondeterministic behavior.

---

## 26. [[Perf] Async Scheduling + Speculative Decoding + Structured Outputs](https://github.com/vllm-project/vllm/pull/29821)


### Base Information

- **PR Number:** #29821
- **Author:** [benchislett](https://github.com/benchislett)
- **Merged By:** [njhill](https://github.com/njhill)
- **Merged time:** 2026-01-06 10:50:37
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29821/files) (8):**
  - `tests/v1/e2e/test_async_scheduling.py`
  - `vllm/v1/core/sched/async_scheduler.py`
  - `vllm/v1/core/sched/interface.py`
  - `vllm/v1/core/sched/output.py`
  - `vllm/v1/core/sched/scheduler.py`
  - `vllm/v1/engine/core.py`
  - `vllm/v1/engine/input_processor.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR enables structured outputs when using speculative decoding with async scheduling. The main changes involve asynchronously copying draft tokens to the CPU to overlap with the next model forward step, handling invalid draft tokens that violate grammar constraints by replacing them with `-1` placeholders, and adjusting acceptance rate calculations to exclude invalid tokens.

**Technical impact**  
The changes introduce a new async copy mechanism for draft tokens using separate CUDA streams, which improves throughput by overlapping data transfer with computation. The scheduler now filters invalid draft tokens via grammar validation and pads rejected tokens with `-1`, ensuring correct bitmask computation. Acceptance rate statistics are adjusted to exclude invalid tokens, maintaining accurate performance metrics.

**Potential risks**  
If the async copy streams are not properly synchronized, it could lead to race conditions or stale draft token data. The use of `-1` placeholders for invalid tokens requires careful handling in downstream bitmask logic to avoid indexing errors. Edge cases like request preemption or early finishing during draft token validation need robust handling to prevent crashes or incorrect outputs.

**Key insights**  
Developers should verify that the async copy streams are correctly managed, especially when requests finish or are preempted mid-copy. The `-1` placeholder convention must be consistently respected across all grammar and bitmask processing code. Performance testing should include varied structured output schemas to ensure the validation overhead doesn’t negate async gains.

---

## 27. [[Bugfix] Fix GLM-4 MoE router logits dtype for data parallel chunking](https://github.com/vllm-project/vllm/pull/31055)


### Base Information

- **PR Number:** #31055
- **Author:** [ReinforcedKnowledge](https://github.com/ReinforcedKnowledge)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-06 09:57:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31055/files) (3):**
  - `vllm/model_executor/layers/fused_moe/config.py`
  - `vllm/model_executor/layers/fused_moe/layer.py`
  - `vllm/model_executor/models/glm4_moe.py`

### Summary

**What changed and why**  
The fix addresses a dtype mismatch in GLM-4 MoE when using data parallel chunking with the PPLX backend. The issue occurs because GLM-4's router gates explicitly use float32, while data parallel chunking pre-allocates buffers using the model's base dtype (typically bfloat16). The changes introduce a `router_logits_dtype` field in the fused MoE configuration to properly handle this mismatch.

**Technical impact**  
This change allows GLM-4 MoE models to work correctly with data parallel chunking optimizations (enabled by `--all2all-backend pplx`). The fix extends the `FusedMoEConfig` to support a separate dtype for router logits, ensuring buffer allocation matches the actual data type produced by the model's gate layer. This maintains compatibility with other MoE architectures while resolving the assertion failure.

**Potential risks**  
The primary risk is that other MoE models might have similar dtype mismatches that were previously hidden because they didn't trigger data parallel chunking. Additionally, if `router_logits_dtype` is not properly set for other models, it could lead to silent precision loss or runtime errors. The default fallback to `in_dtype` should mitigate this for most cases.

**Key insights**  
Developers should ensure that any MoE model with router logits using a different dtype than the model's base dtype explicitly sets `router_logits_dtype`. This fix highlights the importance of considering dtype consistency when enabling advanced parallelization features. The change is minimally invasive and follows the existing configuration pattern, making it easy to maintain.

---

## 28. [make 500: InternalServerError more informative](https://github.com/vllm-project/vllm/pull/20610)


### Base Information

- **PR Number:** #20610
- **Author:** [guicho271828](https://github.com/guicho271828)
- **Merged By:** [yewentao256](https://github.com/yewentao256)
- **Merged time:** 2026-01-06 09:36:24
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/20610/files) (1):**
  - `vllm/v1/engine/async_llm.py`

### Summary

**What changed and why**  
The PR enhances error logging in vLLM's async engine by making 500 Internal Server Error responses more informative. Previously, exceptions were logged with generic messages; now, they include detailed exception class names and messages. This addresses issues like FileNotFoundError being obscured, helping developers debug problems faster.

**Technical impact**  
These changes improve debugging visibility without altering core functionality. The error handling now captures and logs exception details in both ValueError and general Exception cases, providing clearer context in logs while maintaining existing error propagation and recovery mechanisms.

**Potential risks**  
The added try-except block for formatting exception strings could itself raise exceptions (e.g., if `__str__` fails), though this is mitigated by a fallback. There's a minor risk of logging sensitive data from exception messages, but this is consistent with prior behavior.

**Key insights**  
Always include exception details in logs to accelerate debugging. Consider adding structured logging (e.g., JSON) in future iterations for better log parsing. Ensure any fallback error handling is robust to avoid masking the original error.

---

## 29. [[Log] add log about gpu worker init snapshot and requested memory](https://github.com/vllm-project/vllm/pull/29493)


### Base Information

- **PR Number:** #29493
- **Author:** [andyxning](https://github.com/andyxning)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-06 09:32:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29493/files) (3):**
  - `vllm/utils/mem_utils.py`
  - `vllm/v1/worker/gpu_worker.py`
  - `vllm/v1/worker/utils.py`

### Summary

**What changed and why**  
This PR adds detailed logging for GPU worker initialization memory snapshots and requested memory to aid debugging. It introduces a `format_gib` utility function to consistently format memory values in GiB across the codebase and replaces inline calculations with this function.

**Technical impact**  
The changes improve logging consistency and readability by centralizing memory formatting logic. The `MemorySnapshot` class now has a `__repr__` method for better debugging, and memory values in logs are uniformly displayed with two decimal places. The `request_memory` function now returns an integer (ceiling value) instead of a float, ensuring precise memory allocation.

**Potential risks**  
Changing `request_memory` to return a ceiling value could slightly over-allocate memory compared to the previous floating-point result, potentially reducing available memory for other operations. The new logging statements (debug level) may increase log volume, though this is mitigated by using `logger.debug`. Ensure the `format_gib` rounding behavior (two decimal places) aligns with existing precision expectations.

**Key insights**  
Centralizing memory formatting reduces code duplication and improves maintainability. Developers should verify that the ceiling operation in `request_memory` does not cause memory overallocation issues in constrained environments. The enhanced logging will simplify debugging memory-related initialization problems but should be monitored for performance impact in high-frequency logging scenarios.

---

## 30. [[PERF] Speed-up of GDN attention decode part (Qwen3-Next)](https://github.com/vllm-project/vllm/pull/31722)


### Base Information

- **PR Number:** #31722
- **Author:** [vadiklyutiy](https://github.com/vadiklyutiy)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-06 09:32:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31722/files) (1):**
  - `vllm/model_executor/layers/fla/ops/fused_recurrent.py`

### Summary

**What changed and why**  
The change modifies the block size parameter `BV` in the `fused_recurrent_gated_delta_rule_fwd` kernel from `min(triton.next_power_of_2(V), 8)` to `min(triton.next_power_of_2(V), 32)`. This increases the maximum vector block size from 8 to 32, allowing the kernel to process more elements per thread block during the decode phase of Gated Delta Rule attention, specifically for Qwen3-Next models.

**Technical impact**  
Increasing `BV` improves memory access efficiency and computational throughput by enabling wider vector operations. This directly accelerates the attention decode step, as evidenced by the 16.8% end-to-end throughput improvement and consistent speedups across various batch sizes and tensor parallelism configurations (TP=2/4) on both H200 and B200 hardware.

**Potential risks**  
The increased block size may raise register pressure or shared memory usage, potentially affecting occupancy on GPUs with limited resources. While benchmarks show positive results, the change assumes typical decode-phase shapes; extreme edge cases with very small `V` dimensions might see diminished benefits. The assertion `NK == 1` remains unchanged, so any future support for `NK > 1` would require reevaluation.

**Key insights**  
This minor tuning demonstrates significant performance gains in attention decoding, highlighting the sensitivity of Triton kernels to block size parameters. Developers should consider similar optimizations for other recurrent kernels, but must validate across diverse hardware and model configurations. The change is low-risk due to its localized nature and strong benchmarking validation.

---

## 31. [[Attention][2/n] Remove usage of deprecated `seq_lens_cpu` and `num_computed_tokens_cpu` CommonAttentionMetadata properties](https://github.com/vllm-project/vllm/pull/31774)


### Base Information

- **PR Number:** #31774
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-06 09:32:14
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31774/files) (3):**
  - `vllm/v1/attention/backends/gdn_attn.py`
  - `vllm/v1/attention/backends/mamba2_attn.py`
  - `vllm/v1/attention/backends/mamba_attn.py`

### Summary

**What changed and why**  
The changes remove deprecated `seq_lens_cpu` and `num_computed_tokens_cpu` properties from CommonAttentionMetadata usage across three SSM attention backends (gdn_attn, mamba_attn, mamba2_attn). Instead, they now call the `compute_num_computed_tokens()` method, which provides the same data in a more maintainable way.

**Technical impact**  
These changes centralize the computation of computed tokens through a dedicated method, improving code consistency and reducing duplication. The modifications affect how attention metadata is processed for state space model backends, particularly for prefix caching and token computation logic. The device handling is now managed internally by the method rather than explicitly in each backend.

**Potential risks**  
The `compute_num_computed_tokens()` method might have different performance characteristics than the previous direct property access. There's a risk of device mismatch if the method returns tensors on different devices than expected by the calling code. The conditional logic in mamba_attn.py around `num_computed_tokens` initialization could introduce subtle bugs if not all code paths properly initialize this variable.

**Key insights**  
Developers should verify that `compute_num_computed_tokens()` returns tensors on the expected device (likely GPU for most operations). The removal of explicit `.cpu()` calls in some places suggests the method may handle device placement differently. When working with these attention backends, always use the new method rather than accessing deprecated properties, and be mindful of the conditional initialization pattern in mamba_attn.py.

---

## 32. [[Quantization][MoE] remove unused ep logic from moe marlin](https://github.com/vllm-project/vllm/pull/31571)


### Base Information

- **PR Number:** #31571
- **Author:** [jinzhen-lin](https://github.com/jinzhen-lin)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-06 09:07:20
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31571/files) (6):**
  - `csrc/moe/marlin_moe_wna16/kernel.h`
  - `csrc/moe/marlin_moe_wna16/marlin_template.h`
  - `csrc/moe/marlin_moe_wna16/ops.cu`
  - `csrc/moe/torch_bindings.cpp`
  - `vllm/_custom_ops.py`
  - `vllm/model_executor/layers/fused_moe/fused_marlin_moe.py`

### Summary

**What changed and why**  
Removed expert parallelism (EP) logic from the MoE Marlin kernel after a previous PR (#29642) made this support unnecessary. The changes eliminate the `is_ep` parameter and associated conditional code paths that handled invalid expert IDs (`-1`), simplifying the kernel and its call chain.

**Technical impact**  
The kernel no longer needs to skip invalid expert blocks or adjust parallelism based on `expert_id == -1`, reducing branching and computational overhead. The API is streamlined across CUDA kernels, Python bindings, and model execution layers, removing unused parameters and improving code clarity.

**Potential risks**  
If any downstream code still passes or expects the `is_ep` parameter, it will break. Additionally, if expert parallelism is reintroduced in the future, the removed logic would need to be reimplemented. The changes assume all expert IDs are valid, which could cause issues if invalid IDs reappear.

**Key insights**  
This cleanup reduces technical debt and improves kernel performance by removing dead code. Developers should verify that no external dependencies rely on the removed `is_ep` flag and ensure that expert IDs are always valid in the current MoE implementation. Future changes to expert parallelism must reconsider this logic.

---

## 33. [[NemotronH] Use ReplicatedLinear for fc1_latent_proj](https://github.com/vllm-project/vllm/pull/31807)


### Base Information

- **PR Number:** #31807
- **Author:** [roikoren755](https://github.com/roikoren755)
- **Merged By:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged time:** 2026-01-06 08:00:41
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31807/files) (1):**
  - `vllm/model_executor/models/nemotron_h.py`

### Summary

**What changed and why**  
The PR changes the `fc1_latent_proj` layer in NemotronH's latent MoE from `ColumnParallelLinear` to `ReplicatedLinear`. This eliminates the synchronization overhead (all-gather) associated with tensor parallelism, as profiling showed significant performance costs for this relatively small layer.

**Technical impact**  
This reduces communication overhead in distributed training/inference, improving throughput. Memory usage may increase slightly due to layer replication across devices, but the trade-off is favorable given the layer's small size. The `gather_output=True` parameter is removed as `ReplicatedLinear` inherently avoids output gathering.

**Potential risks**  
If the layer size grows in future model versions, the increased memory footprint from replication could become non-negligible. The change assumes the layer remains small; if this assumption changes, performance may degrade. There is also a risk if other parts of the system implicitly depend on the previous tensor-parallel behavior.

**Key insights**  
Always profile synchronization overhead in distributed settings—small layers can become bottlenecks. Consider using `ReplicatedLinear` for layers where communication costs outweigh memory duplication. Monitor memory usage if layer dimensions evolve, and ensure the change aligns with the overall model parallelism strategy.

---

## 34. [[MoE Refactor][14/N] Clean Up FI Quant Config Smuggling](https://github.com/vllm-project/vllm/pull/31593)


### Base Information

- **PR Number:** #31593
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-06 07:47:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31593/files) (7):**
  - `tests/kernels/moe/test_flashinfer.py`
  - `vllm/model_executor/layers/fused_moe/config.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py`
  - `vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py`
  - `vllm/model_executor/layers/quantization/fp8.py`
  - `vllm/model_executor/layers/quantization/modelopt.py`
  - `vllm/model_executor/layers/quantization/utils/flashinfer_utils.py`

### Summary

**What changed and why**  
This PR refactors FlashInfer FP8 MoE quantization scale handling to clean up "smuggling" of scales through global scale parameters. It introduces explicit scale registration functions (`register_scales_for_trtllm_fp8_per_tensor_moe`, `make_fp8_moe_alpha_scales_for_fi`) and removes the generic `register_moe_scaling_factors`. The changes ensure proper scale computation for both CUTLASS and TensorRT-LLM backends, particularly for per-tensor quantization.

**Technical impact**  
The refactor clarifies scale propagation by separating logic for different backends. FlashInfer CUTLASS per-tensor now uses combined dequantization scales (`g1_alphas = w_scale * a_scale`), while TensorRT-LLM backend registers scales directly on the layer. The modular kernel abstraction is bypassed for TensorRT-LLM, and dynamic per-token quantization is explicitly unsupported for FlashInfer FP8 MoE.

**Potential risks**  
If scale registration fails for TensorRT-LLM (e.g., missing `output*_scales_scalar` attributes), the kernel may crash. The removal of `register_moe_scaling_factors` could break existing code relying on its side effects. Additionally, the assumption that `w2_input_scale_inv` is always needed for TensorRT-LLM may not hold for all model configurations.

**Key insights**  
Developers should use the new explicit scale registration functions instead of the removed generic helper. FlashInfer FP8 MoE does not support dynamic per-token quantization—static or per-tensor schemes must be used. Ensure that all required scale attributes are present when using TensorRT-LLM backend, and verify that scale computations align with the backend’s expectations (e.g., `g1_alphas` for CUTLASS).

---

## 35. [[MoE Refactor] Add Temporary Integration Tests - H100/B200](https://github.com/vllm-project/vllm/pull/31759)


### Base Information

- **PR Number:** #31759
- **Author:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged By:** [mgoin](https://github.com/mgoin)
- **Merged time:** 2026-01-06 07:34:17
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31759/files) (30):**
  - `.buildkite/test-pipeline.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-Fp8-ModelOpt-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-Fp8-ModelOpt-fi-trtllm.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-Fp8-ModelOpt-marlin.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-Fp8-ModelOpt-triton.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Mixtral-8x7B-Fp8-AutoFp8-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Mixtral-8x7B-Fp8-AutoFp8-triton.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-deepgemm.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-fi-trtllm.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-marlin.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-triton.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Block-deepgemm.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Block-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Block-marlin.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Block-vllm-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Channel-marlin.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Channel-vllm-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-CT-fi-cutlass-dp-ep.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-CT-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-CT-fi-trtllm.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-CT-marlin.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-CT-vllm-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-ModelOpt-fi-cutlass-dp-ep.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-ModelOpt-fi-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-ModelOpt-fi-trtllm.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-ModelOpt-marlin.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-ModelOpt-vllm-cutlass.yaml`
  - `tests/evals/gsm8k/configs/moe-refactor/config-b200.txt`
  - `tests/evals/gsm8k/configs/moe-refactor/config-h100.txt`

### Summary

**What changed and why**  
This PR adds temporary integration tests for MoE (Mixture of Experts) refactoring work. It creates new CI pipeline jobs for H100 and B200 GPUs that run GSM8K correctness tests with various MoE model configurations and backend implementations (FlashInfer, DeepGEMM, Marlin, Triton). The purpose is to validate the MoE refactor changes before they are merged, but these tests are marked as optional and temporary due to their compute-intensive nature.

**Technical impact**  
The changes add 30 new configuration files for testing different MoE model variants (Llama-4-Scout, Qwen3-30B-A3B, Mixtral-8x7B) with various precision formats (FP8, NVFP4) and backend configurations. Two new CI pipeline steps are added that reference these configurations, but they are marked as optional to avoid blocking regular CI runs. This creates a comprehensive test matrix for MoE functionality across different hardware and software backends.

**Potential risks**  
The temporary nature of these tests creates a risk that they might not be properly maintained or removed after the refactor is complete. Some configurations are commented out (like Mixtral-8x7B-Fp8-AutoFp8-fi-cutlass.yaml), indicating incomplete test coverage. The compute-intensive nature could impact CI resource availability if not properly managed, though they are marked optional.

**Key insights**  
Developers should note that these tests are specifically for MoE refactoring validation and are not part of the permanent test suite. The configuration files demonstrate extensive testing of different MoE backends and precision formats, which is valuable for understanding the MoE ecosystem. The PR author should ensure these temporary tests are removed once the refactoring is complete to avoid CI pipeline clutter.

---

## 36. [[Bugfix]: Fix cross attention backend selection for Turing GPU](https://github.com/vllm-project/vllm/pull/31806)


### Base Information

- **PR Number:** #31806
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-06 07:15:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31806/files) (1):**
  - `vllm/attention/layers/cross_attention.py`

### Summary

**What changed and why**  
The fix moves the `attn_type` validation earlier and passes `attn_type=AttentionType.ENCODER_DECODER` to `get_attn_backend()`. This ensures the correct attention backend is selected for cross-attention on Turing GPUs, preventing a `NotImplementedError` when FlashInfer is the default backend.

**Technical impact**  
This change ensures cross-attention layers properly initialize by explicitly specifying the encoder-decoder attention type during backend selection. It maintains backward compatibility while fixing the initialization flow for models like Whisper that use cross-attention on Turing architecture GPUs.

**Potential risks**  
If other code paths rely on the previous order of operations, subtle initialization issues could occur. The change assumes `attn_type` will always be `ENCODER_DECODER` for cross-attention, which should hold but warrants verification in all usage contexts.

**Key insights**  
Always pass explicit attention types when selecting backends to avoid default backend mismatches. The fix highlights the importance of proper parameter propagation in attention layer initialization, particularly for specialized hardware configurations like Turing GPUs.

---

## 37. [[LoRA]Disable linear LoRA  kernel PDL](https://github.com/vllm-project/vllm/pull/31777)


### Base Information

- **PR Number:** #31777
- **Author:** [jeejeelee](https://github.com/jeejeelee)
- **Merged By:** [jeejeelee](https://github.com/jeejeelee)
- **Merged time:** 2026-01-06 07:12:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31777/files) (4):**
  - `docs/features/lora.md`
  - `vllm/lora/ops/triton_ops/fused_moe_lora_op.py`
  - `vllm/lora/ops/triton_ops/lora_expand_op.py`
  - `vllm/lora/ops/triton_ops/lora_shrink_op.py`

### Summary

**What changed and why**  
This PR temporarily disables PDL (Persistent Data Loading) for linear LoRA kernels. The change was made because a base model GEMM operator runs between the two LoRA operators, preventing back-to-back kernel execution and making PDL ineffective while negatively impacting performance.

**Technical impact**  
The modifications affect three LoRA kernel files by hardcoding `use_gdc=False` instead of dynamically checking device support via `supports_pdl()`. This ensures PDL is disabled for both shrink and expand operations in linear and fused MoE LoRA kernels, potentially improving throughput by avoiding unnecessary PDL overhead.

**Potential risks**  
Disabling PDL unconditionally may reduce performance on devices where PDL is beneficial when kernels can run back-to-back. The change also removes device-specific adaptability, which could affect future optimizations or hardware support. Additionally, the fused MoE LoRA changes introduce a new `use_gdc` parameter that must be consistently passed through call chains.

**Key insights**  
Benchmark results show minor performance improvements (e.g., TPOT decreased from 33.29ms to 32.95ms). Consider adding a configuration flag to re-enable PDL selectively once the kernel launch ordering issue is resolved. Ensure all call sites for fused MoE LoRA functions are updated to pass the new `use_gdc` parameter.

---

## 38. [[Model] rename use_pad_token to use_sep_token](https://github.com/vllm-project/vllm/pull/31784)


### Base Information

- **PR Number:** #31784
- **Author:** [noooop](https://github.com/noooop)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-06 06:16:05
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31784/files) (5):**
  - `examples/pooling/score/convert_model_to_seq_cls.py`
  - `tests/entrypoints/pooling/score/test_utils.py`
  - `vllm/config/model.py`
  - `vllm/entrypoints/score_utils.py`
  - `vllm/model_executor/models/adapters.py`

### Summary

**What changed and why**  
The PR renames the configuration parameter `use_pad_token` to `use_sep_token` across the codebase. This change clarifies the parameter's purpose—it controls whether a separating token is used when processing text pairs for scoring/reranking tasks, not padding behavior. The update includes backward compatibility with a deprecation warning for the old parameter.

**Technical impact**  
This change affects model configuration handling, tokenization logic, and conversion scripts for sequence classification models. The core behavior remains the same: when `use_sep_token=True` (default for cross-encoders), the tokenizer processes text pairs with a separator; when `False` (default for LLM-as-reranker models), texts are concatenated without a separator. The parameter name now accurately reflects its function.

**Potential risks**  
The deprecation warning for `use_pad_token` may cause confusion if users or downstream code still reference the old parameter. There's a risk that some configurations might not be updated, leading to inconsistent behavior. Additionally, any external scripts or documentation referencing `use_pad_token` will need updates to avoid errors or warnings.

**Key insights**  
This is a semantic improvement that aligns the parameter name with its actual functionality. Developers should update any configurations or code using `use_pad_token` to `use_sep_token`. The backward compatibility layer ensures smooth transition but should be monitored for removal in future releases. Ensure all related documentation and examples reflect this change.

---

## 39. [[Frontend] Support GLM-4.5 / GLM-4.7 with enable_thinking: false](https://github.com/vllm-project/vllm/pull/31788)


### Base Information

- **PR Number:** #31788
- **Author:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-06 05:53:22
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31788/files) (3):**
  - `vllm/reasoning/deepseek_v3_reasoning_parser.py`
  - `vllm/reasoning/glm4_moe_reasoning_parser.py`
  - `vllm/reasoning/holo2_reasoning_parser.py`

### Summary

**What changed and why**  
The changes modify reasoning parsers to properly handle `enable_thinking: false` for GLM-4.5/GLM-4.7 models. Specifically, they adjust how chat template kwargs are accessed (using `.get()` instead of `.pop()`) and update inheritance/condition logic to respect both `thinking` and `enable_thinking` flags.

**Technical impact**  
These updates ensure that when `enable_thinking` is set to `false`, the reasoning parser defaults to an `IdentityReasoningParser` (no reasoning parsing) instead of forcing reasoning mode. The GLM-4 MOE parser now inherits from `Holo2ReasoningParser` rather than `DeepSeekR1ReasoningParser`, aligning with the correct parsing hierarchy.

**Potential risks**  
If `thinking` and `enable_thinking` are both provided with conflicting values, the current logic (`thinking = thinking and enable_thinking`) may produce unintended behavior. Additionally, the switch from `.pop()` to `.get()` could affect state management if other code depends on modifying the original `chat_kwargs` dictionary.

**Key insights**  
Developers should verify that the `thinking`/`enable_thinking` flag interactions are consistent across all model types. The inheritance change for GLM-4 MOE should be validated to ensure it doesn’t break existing parsing behavior. Consider adding explicit documentation or validation for chat template kwargs to prevent misuse.

---

## 40. [[Bugfix]: avoid overriding audio/text kwargs (Qwen3-Omni)](https://github.com/vllm-project/vllm/pull/31790)


### Base Information

- **PR Number:** #31790
- **Author:** [Jzz1943](https://github.com/Jzz1943)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-06 04:59:17
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31790/files) (1):**
  - `vllm/model_executor/models/qwen3_omni_moe_thinker.py`

### Summary

**What changed and why**  
The fix addresses a bug where `audio_kwargs` and `text_kwargs` could be unintentionally overridden during keyword argument restructuring for older Transformers versions (<4.58.0). Previously, these nested dictionaries were replaced entirely; now they are initialized if missing and updated only with a default `truncation` value using `setdefault()`, preserving any user-provided nested arguments.

**Technical impact**  
This change ensures backward compatibility with Transformers <4.58.0 while maintaining the intended precedence: user-supplied `audio_kwargs` and `text_kwargs` take priority over defaults. The restructuring logic now correctly merges defaults with existing configurations instead of overwriting them, which affects how multimodal inputs are processed in the Qwen3-Omni model.

**Potential risks**  
If `audio_kwargs` or `text_kwargs` already contain a `truncation` key, the `setdefault` will leave it unchanged, which might conflict with the intended default behavior from `mm_kwargs` or `tok_kwargs`. Additionally, the initialization with empty dicts when these keys are absent could introduce unexpected empty structures if not handled downstream.

**Key insights**  
Use `setdefault()` for safe default assignment in nested configurations to avoid overriding user-provided values. Always verify that nested dictionary initialization aligns with the expected structure in subsequent processing steps. This pattern is a good practice for maintaining flexible and backward-compatible API surfaces.

---

## 41. [[Misc] Implement `TokenizerLike.convert_tokens_to_ids`](https://github.com/vllm-project/vllm/pull/31796)


### Base Information

- **PR Number:** #31796
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-06 04:08:22
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31796/files) (3):**
  - `vllm/tokenizers/deepseek_v32.py`
  - `vllm/tokenizers/mistral.py`
  - `vllm/tokenizers/protocol.py`

### Summary

**What changed and why**  
The PR adds a `convert_tokens_to_ids` method to the `TokenizerLike` protocol and implements it in the `deepseek_v32` and `mistral` tokenizer classes. This change addresses a compatibility issue where multi-modal processors expect this method to exist on tokenizer instances, fixing GitHub issue #31792.

**Technical impact**  
This change extends the `TokenizerLike` interface to include a commonly used tokenizer method, improving compatibility with components that rely on the standard tokenizer API. The implementations delegate directly to the underlying tokenizer objects (`self.tokenizer` or `self.transformers_tokenizer`), maintaining consistency with existing behavior.

**Potential risks**  
The overload decorators provide type hints for both string and list inputs, but runtime type checking may be limited. If underlying tokenizers have different behavior for edge cases (empty strings/lists, invalid tokens), those behaviors will be propagated. There's also a risk that other tokenizer implementations in the codebase may need similar updates if they also implement `TokenizerLike`.

**Key insights**  
The change is minimal and follows the existing pattern of delegating to wrapped tokenizers. Developers should ensure all `TokenizerLike` implementations include this method. Consider adding a test to verify consistent behavior across different tokenizer implementations, especially for edge cases. The overload signatures provide good type safety for callers.

---

## 42. [[Bugfix] Fix torch.compile error for DP + MoE on CPU Backend](https://github.com/vllm-project/vllm/pull/31650)


### Base Information

- **PR Number:** #31650
- **Author:** [kzwrime](https://github.com/kzwrime)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-01-06 04:06:21
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31650/files) (1):**
  - `vllm/model_executor/layers/fused_moe/layer.py`

### Summary

**What changed and why**  
The change reorders the conditions in the `post_quant_allgather` boolean expression. Specifically, it moves `has_flashinfer_trtllm_fused_moe()` from the first position to the last position in the logical AND chain. This prevents an early evaluation failure when the function call is unsupported on the CPU backend, allowing the other conditions (like quantization and distributed parallelism) to be checked first.

**Technical impact**  
This fix resolves a torch.compile error for MoE models on CPU backend with data parallelism (`-dp 2`). By reordering the conditions, the code avoids invoking `has_flashinfer_trtllm_fused_moe()` when it's not applicable (e.g., on CPU), ensuring the expression evaluates correctly without raising backend-specific errors.

**Potential risks**  
If `has_flashinfer_trtllm_fused_moe()` has side effects or is required for correctness in some configurations, moving it to the end could delay its execution. However, since it's a predicate check, this risk is minimal. The change assumes the function is safe to call in all contexts where the preceding conditions are met.

**Key insights**  
Always place backend-specific or potentially failing checks later in conditional chains to allow earlier, safer conditions to short-circuit evaluation. This pattern improves robustness across different hardware and compilation settings. Developers should verify that reordering doesn't alter the logical meaning of the expression, which appears preserved here.

---

## 43. [[Attention][1/n] Remove usage of deprecated `seq_lens_cpu` and `num_computed_tokens_cpu` CommonAttentionMetadata properties](https://github.com/vllm-project/vllm/pull/31773)


### Base Information

- **PR Number:** #31773
- **Author:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-06 04:05:17
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31773/files) (9):**
  - `tests/v1/attention/test_attention_backends.py`
  - `tests/v1/attention/test_mla_backends.py`
  - `tests/v1/attention/test_sparse_mla_backends.py`
  - `vllm/v1/attention/backends/flashinfer.py`
  - `vllm/v1/attention/backends/flex_attention.py`
  - `vllm/v1/attention/backends/mla/common.py`
  - `vllm/v1/attention/backends/mla/flashmla_sparse.py`
  - `vllm/v1/attention/backends/triton_attn.py`
  - `vllm/v1/attention/backends/utils.py`

### Summary

**What changed and why**  
This PR removes direct usage of the deprecated `seq_lens_cpu` and `num_computed_tokens_cpu` properties from `CommonAttentionMetadata`. Instead, it replaces them with calls to `seq_lens.cpu()` and a new method `compute_num_computed_tokens()`, which computes the value on the device and caches it. The changes update tests and multiple attention backends to align with the new interface.

**Technical impact**  
The changes shift computation of `num_computed_tokens` to the device (GPU) via `compute_num_computed_tokens()`, reducing CPU-GPU synchronization overhead. The `seq_lens_cpu` property is replaced with explicit `.cpu()` calls, making data movement more explicit. A caching mechanism (`_num_computed_tokens_cache`) is introduced to avoid redundant computations.

**Potential risks**  
If `compute_num_computed_tokens()` is called before `seq_lens` or `query_start_loc` are properly initialized, it may lead to incorrect results or errors. The caching assumes that `seq_lens` and `query_start_loc` do not change after the first call, which must hold true for the metadata's lifecycle. There is also a risk of subtle bugs if the cached tensor is modified inadvertently.

**Key insights**  
Developers should use `compute_num_computed_tokens()` for device-side computations and `seq_lens.cpu()` for CPU tensors, avoiding the deprecated properties. Ensure that `seq_lens` and `query_start_loc` are stable before caching. The changes improve performance by minimizing synchronization, but care is needed to maintain correctness in asynchronous or state-changing scenarios.

---

## 44. [[Chore] Cleanup `mem_utils.py`](https://github.com/vllm-project/vllm/pull/31793)


### Base Information

- **PR Number:** #31793
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-06 03:56:00
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31793/files) (1):**
  - `vllm/utils/mem_utils.py`

### Summary

**What changed and why**  
This PR cleans up the `mem_utils.py` file by ensuring `MemoryProfilingResult` snapshots use a consistent device and removing unnecessary `noqa` comments. Key changes include fixing a typo in an assertion message, converting `weights_memory` from `float` to `int`, and initializing `before_profile` and `after_profile` snapshots with the same device as `before_create` via `__post_init__`.

**Technical impact**  
The changes improve type consistency (e.g., `weights_memory` as `int`) and ensure memory snapshots are device-aligned, preventing potential device mismatches in profiling. The `memory_profiling` context manager now initializes `MemoryProfilingResult` with explicit parameters and resets peak stats on the correct device, enhancing reliability in multi-GPU environments.

**Potential risks**  
If `before_create.device_` is invalid or unset during `__post_init__`, snapshot initialization could fail. The removal of `noqa` comments assumes linting issues are resolved, but any unresolved warnings could reappear. There’s also a minor risk that the `int` conversion of `weights_memory` might truncate values if non-integer inputs were previously allowed.

**Key insights**  
Ensure `before_create` has a valid `device_` attribute before `MemoryProfilingResult` initialization. Verify that all `noqa` removals don’t introduce new linting errors. The explicit device handling in `torch.cuda.reset_peak_memory_stats()` is a positive change for multi-GPU support, but cross-check that device indexing aligns with broader system usage.

---

## 45. [[Doc] Fix format of multimodal_inputs.md](https://github.com/vllm-project/vllm/pull/31800)


### Base Information

- **PR Number:** #31800
- **Author:** [BlankRH](https://github.com/BlankRH)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-06 03:30:24
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31800/files) (1):**
  - `docs/features/multimodal_inputs.md`

### Summary

**What changed and why**  
The PR fixes formatting issues in the multimodal_inputs.md documentation by converting Python code blocks into collapsible sections using the `??? code` syntax. This improves readability for long code examples in the documentation.

**Technical impact**  
These changes affect only documentation formatting and have no impact on code functionality, system behavior, or architecture. The modifications make the documentation more user-friendly by allowing code examples to be collapsed/expanded as needed.

**Potential risks**  
The main risk is potential syntax errors in the markdown formatting if the `??? code` syntax isn't properly supported by the documentation generator. There's also a minor risk of inconsistent formatting if other documentation files don't follow the same pattern.

**Key insights**  
This is a documentation-only change that improves user experience. Developers should ensure the documentation generator supports the new syntax and consider applying similar formatting to other long code examples in the documentation for consistency. The changes are minimal and focused on presentation rather than content.

---

## 46. [[CI] Increase the MTEB_EMBED_TOL threshold to 5e-4.](https://github.com/vllm-project/vllm/pull/31797)


### Base Information

- **PR Number:** #31797
- **Author:** [noooop](https://github.com/noooop)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-01-06 03:30:05
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31797/files) (5):**
  - `tests/models/language/pooling_mteb_test/mteb_embed_utils.py`
  - `tests/models/language/pooling_mteb_test/test_baai.py`
  - `tests/models/language/pooling_mteb_test/test_gte.py`
  - `tests/models/language/pooling_mteb_test/test_jina.py`
  - `tests/models/language/pooling_mteb_test/test_st_projector.py`

### Summary

**What changed and why**  
The PR increases the tolerance threshold (`MTEB_EMBED_TOL`) from 1e-4 to 5e-4 in the MTEB embedding test utilities. This change addresses flakiness in the `test_embed_models_mteb` test, which has been failing intermittently due to minor numerical variations in embedding outputs. Additionally, the `dtype="float32"` specification was removed from several model configuration entries across multiple test files, likely to simplify test setups or align with default behavior.

**Technical impact**  
Increasing the tolerance threshold makes the embedding comparison tests less sensitive to small numerical differences, which can arise from hardware variations, compiler optimizations, or minor implementation changes. Removing explicit `dtype` parameters may allow tests to use default data types, potentially reducing configuration overhead and ensuring consistency across different testing environments.

**Potential risks**  
A higher tolerance threshold could mask genuine regressions in model performance or embedding quality, as larger numerical deviations might go undetected. Removing explicit `dtype` specifications might lead to unexpected behavior if models default to different data types in various environments, though this risk appears low given the context.

**Key insights**  
This change is a pragmatic fix for test stability, but developers should monitor whether the increased tolerance is appropriate for the intended precision of embedding models. Consider documenting the rationale for the threshold adjustment and periodically reviewing test results to ensure it doesn’t hide significant issues. The removal of redundant `dtype` parameters is a positive cleanup, but verify that all affected models function correctly with implicit defaults.

---

## 47. [[Misc] Use `deprecated` for `seed_everything`](https://github.com/vllm-project/vllm/pull/31780)


### Base Information

- **PR Number:** #31780
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-06 03:29:55
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31780/files) (12):**
  - `benchmarks/kernels/benchmark_activation.py`
  - `benchmarks/kernels/benchmark_layernorm.py`
  - `benchmarks/kernels/benchmark_moe.py`
  - `benchmarks/kernels/benchmark_moe_permute_unpermute.py`
  - `benchmarks/kernels/benchmark_mrope.py`
  - `benchmarks/kernels/benchmark_paged_attention.py`
  - `benchmarks/kernels/benchmark_quant.py`
  - `benchmarks/kernels/benchmark_reshape_and_cache.py`
  - `benchmarks/kernels/benchmark_reshape_and_cache_flash.py`
  - `benchmarks/kernels/benchmark_silu_mul_fp8_quant.py`
  - `docs/design/plugin_system.md`
  - `vllm/platforms/interface.py`

### Summary

**What changed and why**  
This PR replaces deprecated `current_platform.seed_everything()` calls with `set_random_seed()` from `vllm.utils.torch_utils` across multiple benchmark files. The `seed_everything` method in the platform interface is now decorated with `@deprecated`, and its removal timeline is extended from v0.14.0 to v0.15.0.

**Technical impact**  
The changes centralize random seed management by removing platform-specific dependencies in benchmark code. This simplifies the codebase and aligns with the deprecation strategy, ensuring benchmarks use the recommended utility function. The platform interface remains backward-compatible but now emits formal deprecation warnings.

**Potential risks**  
If any external plugins or custom platforms rely on `seed_everything`, they will encounter deprecation warnings and need migration. The extended removal timeline (v0.15.0) reduces immediate breakage risk but requires downstream updates before the final removal.

**Key insights**  
Developers should adopt `set_random_seed()` for all new code and update existing usage to avoid future breaks. The PR successfully updates internal benchmarks, but external integrations should be audited for similar deprecated calls. The deprecation decorator provides clearer warnings than the previous log-based approach.

---

## 48. [[cpu][bench] Add CPU paged attention benchmarks](https://github.com/vllm-project/vllm/pull/31720)


### Base Information

- **PR Number:** #31720
- **Author:** [fadara01](https://github.com/fadara01)
- **Merged By:** [bigPYJ1151](https://github.com/bigPYJ1151)
- **Merged time:** 2026-01-06 02:57:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31720/files) (1):**
  - `benchmarks/kernels/cpu/benchmark_cpu_attn.py`

### Summary

**What changed and why**  
Added a new CPU paged attention benchmark script (`benchmark_cpu_attn.py`) to measure the performance of CPU attention kernels with paged KV cache. This addresses issue #30374 by providing a tool to evaluate CPU attention performance across different configurations (batch sizes, sequence lengths, data types, instruction sets, etc.).

**Technical impact**  
The benchmark integrates with existing CPU attention operations (`cpu_attention_with_kv_cache`, `cpu_attn_reshape_and_cache`) and supports key features like sliding window attention, sink tokens, and KV split. It provides detailed timing statistics (min, max, mean, std, median) and uses cached random tensors to reduce initialization overhead during repeated runs.

**Potential risks**  
The benchmark assumes compatibility between the selected ISA, block size, and dtype (via `_get_attn_isa`), which may fail if unsupported combinations are used. Randomly generated block tables could map to invalid blocks if `num_blocks` is too small relative to `max_kv_len`. The fixed tensor cache size (LRU with maxsize=128) might not cover all possible benchmark configurations in long-running sessions.

**Key insights**  
This is a well-structured benchmark that validates CPU attention performance in realistic paged scenarios. Developers should ensure the benchmark parameters align with production use cases and verify ISA support on the target hardware. Consider adding validation for `num_blocks` sufficiency and expanding the tensor cache if benchmarking many unique configurations.

---

## 49. [[Chore] Remove more V0 dead code from `sequence.py`](https://github.com/vllm-project/vllm/pull/31783)


### Base Information

- **PR Number:** #31783
- **Author:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-06 02:25:14
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31783/files) (2):**
  - `vllm/outputs.py`
  - `vllm/sequence.py`

### Summary

**What changed and why**  
Removed the deprecated `RequestMetrics` class from `vllm/sequence.py` and updated `vllm/outputs.py` to no longer import or accept it as a parameter type. This is part of ongoing cleanup to eliminate dead code from the V0 API.

**Technical impact**  
The `RequestOutput` class now only accepts `RequestStateStats` for the `metrics` parameter, simplifying the type signature. This change reduces code complexity and removes unused data structures, but requires any remaining code that still uses `RequestMetrics` to migrate to `RequestStateStats`.

**Potential risks**  
If any internal or external code still references `RequestMetrics`, it will break after this change. The PR description mentions moving `IntermediateTensors` elsewhere, but doesn't address potential dependencies on the removed constants (`VLLM_TOKEN_ID_ARRAY_TYPE`, `VLLM_INVALID_TOKEN_ID`).

**Key insights**  
This is a straightforward cleanup, but developers should verify no other modules import `RequestMetrics`. The removal of the two constants should be checked for any hidden usage. Future PRs should ensure `IntermediateTensors` relocation is completed to maintain code organization.

---

## 50. [[Bugfix][CI/Build] Fix failing pooling models test due to Triton kernel accuracy diff](https://github.com/vllm-project/vllm/pull/31776)


### Base Information

- **PR Number:** #31776
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-01-06 00:44:23
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31776/files) (1):**
  - `tests/models/language/pooling/test_token_classification.py`

### Summary

**What changed and why**  
The PR increases the tolerance thresholds in a pooling model test by modifying `assert torch.allclose` to `torch.testing.assert_close` with adjusted `atol` (from 1e-2 to 1.2e-2) and added `rtol` (1e-3). This addresses minor accuracy differences (~0.001) between Triton kernel and FlexAttention backends that caused CI failures.

**Technical impact**  
This change only affects test validation, not production code. It accommodates numerical variations in kernel implementations while maintaining test integrity. The switch to `torch.testing.assert_close` provides more explicit tolerance control compared to `torch.allclose`.

**Potential risks**  
Overly relaxed tolerances could mask genuine regressions in model outputs. The inability to reproduce locally suggests environment-specific differences (e.g., hardware, library versions) that may resurface. Future kernel changes might require further tolerance adjustments.

**Key insights**  
Tests should balance strictness with practical numerical variations across backends. Consider documenting tolerance rationale for future maintainers. If failures persist, investigate root causes in kernel implementations rather than continually adjusting tolerances.

---

## 51. [[Models]: Use `MMEncoderAttention` for MoonViT](https://github.com/vllm-project/vllm/pull/31738)


### Base Information

- **PR Number:** #31738
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-06 00:00:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31738/files) (2):**
  - `vllm/model_executor/models/kimi_vl.py`
  - `vllm/model_executor/models/moonvit.py`

### Summary

**What changed and why**  
This PR updates the MoonViT model implementation to use the standardized `MMEncoderAttention` layer instead of custom attention functions, and adds proper tensor parallel support for vision models. The changes replace manual flash attention/SDPA implementations with vLLM's unified attention layer while enabling both weight and data parallel modes for multimodal encoder tensor parallelism.

**Technical impact**  
The refactoring centralizes attention logic into vLLM's shared `MMEncoderAttention` layer, improving code maintainability and ensuring consistent attention behavior across vision-language models. The introduction of proper tensor parallel linear layers (`ColumnParallelLinear`, `RowParallelLinear`, `QKVParallelLinear`) enables efficient distributed execution for vision encoders, with configurable parallelization modes via `mm_encoder_tp_mode`.

**Potential risks**  
The attention interface change could introduce subtle behavioral differences in attention computations compared to the previous custom implementations. The tensor parallel support adds complexity that requires thorough testing across different parallelization configurations. There's also a risk of performance regression if the new attention layer isn't optimized for the specific patterns of vision transformer workloads.

**Key insights**  
Developers should verify that the new attention implementation produces numerically equivalent results to the previous version, especially for edge cases. The tensor parallel support enables scaling vision models but requires careful configuration of `mm_encoder_tp_mode` based on workload characteristics. Future vision model implementations should follow this pattern of using standardized attention layers and tensor parallel primitives.

---

