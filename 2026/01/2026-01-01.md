# vLLM Merged PR Report

**Report Date:** 2026-01-01 PST

**Total Merged PRs:** 11

---

## 1. [[Bugfix] Fix weight_loader v1 block scale](https://github.com/vllm-project/vllm/pull/31103)


### Base Information

- **PR Number:** #31103
- **Author:** [kyuyeunk](https://github.com/kyuyeunk)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-01 21:14:10
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31103/files) (1):**
  - `vllm/model_executor/layers/linear.py`

### Summary

**What changed and why**  
The PR fixes a bug in `weight_loader_v1` where `shard_size` and `shard_offset` were incorrectly calculated for `BlockQuantScaleParameter` types. The issue occurred because the division by `tp_size` (tensor parallelism size) was applied before adjusting for block quantization scaling. The fix introduces a helper function `adjust_block_scale_shard` and ensures block-scale adjustments happen before TP division in all relevant loader variants (v1, v2, and QKV-specific cases).

**Technical impact**  
This change ensures consistent shard calculation logic across all weight loader variants by centralizing block-scale adjustment into a reusable function. It corrects the order of operations—block scaling now precedes TP division—which is critical for proper tensor parallelism with quantized models. The fix maintains backward compatibility for non-quantized parameters while aligning v1 behavior with the already-correct v2 implementation.

**Potential risks**  
If `weight_block_size` is not properly initialized for `BlockQuantScaleParameter` instances, the new `adjust_block_scale_shard` function will raise an assertion error. There’s also a risk of regression in non-quantized scenarios if the helper function is incorrectly invoked. Additionally, the changes affect multiple loader paths, so thorough testing across different quantization methods and model architectures is essential.

**Key insights**  
The refactoring improves code maintainability by deduplicating block-scale logic. Developers should verify that `weight_block_size` is always set when using block quantization. The fix highlights the importance of operation ordering in distributed weight loading—block scaling must precede TP division. Consider adding unit tests that cover edge cases with various quantization configurations and tensor parallelism degrees.

---

## 2. [[Bugfix][Hardware][AMD] Fix last_page_len calculation in AITER MLA decode](https://github.com/vllm-project/vllm/pull/31282)


### Base Information

- **PR Number:** #31282
- **Author:** [c0de128](https://github.com/c0de128)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-01 21:14:00
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31282/files) (1):**
  - `vllm/v1/attention/backends/mla/rocm_aiter_mla.py`

### Summary

**What changed and why**  
The fix addresses an incorrect calculation of `paged_kv_last_page_len` in the ROCm AITER MLA decode path. Previously, this value was set to the full sequence length, but since the kernel uses a block size of 1 (each page holds exactly one token), the last page length should always be 1. The solution pre-initializes a persistent buffer of ones and reuses slices, eliminating the buggy per-request computation.

**Technical impact**  
This change ensures correct attention score calculations and prevents potential out-of-bounds memory access in the MLA decode kernel. It also improves efficiency by replacing runtime computations with a pre-initialized buffer, aligning the AITER backend's behavior with other backends like FlashInfer (where `last_page_len` is derived via modulo with page size).

**Potential risks**  
If the kernel's block size assumption (always 1) changes in the future, this hardcoded approach could break. Additionally, the fix relies on correct buffer slicing; any off-by-one errors in `num_reqs` could lead to incorrect buffer sizes or values. The removal of the conditional `torch.where` may affect edge cases with zero-length sequences, though the buffer initialization to 1s likely handles this.

**Key insights**  
Always validate kernel assumptions (like block size) when optimizing buffer usage. Consider adding a comment or assertion that the block size is 1 to prevent future regressions. The pre-initialized buffer pattern is efficient but should be documented to clarify its dependency on fixed kernel parameters. Testing with prime-length sequences effectively exposed the original bug.

---

## 3. [Remove unused `use_marlin` variable in `Mxfp4MoEMethod`](https://github.com/vllm-project/vllm/pull/31549)


### Base Information

- **PR Number:** #31549
- **Author:** [vsourirajan](https://github.com/vsourirajan)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-01 21:13:37
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31549/files) (1):**
  - `vllm/model_executor/layers/quantization/mxfp4.py`

### Summary

**What changed and why**  
Removed the unused `self.use_marlin` variable from the `Mxfp4MoEMethod` class. The variable was previously set to `self.mxfp4_backend == Mxfp4Backend.MARLIN`, but all logic checking for the Marlin backend now directly uses the `self.mxfp4_backend` comparison.

**Technical impact**  
This is a minor cleanup change that removes dead code. It reduces the class's memory footprint by one attribute and eliminates a redundant boolean variable, making the code slightly more maintainable by having a single source of truth for backend checks.

**Potential risks**  
The risk is very low. However, developers should verify that no other code (e.g., in subclasses or via reflection) was inadvertently relying on the `use_marlin` attribute. The direct equality check (`self.mxfp4_backend == Mxfp4Backend.MARLIN`) is functionally equivalent.

**Key insights**  
This is a straightforward code hygiene improvement. When performing similar cleanups, always search for any indirect references to the removed symbol. The PR description correctly notes that the backend state is now solely determined by `self.mxfp4_backend`.

---

## 4. [[Bugfix] Fix activation quantization for compressed-tensors W4A16](https://github.com/vllm-project/vllm/pull/31572)


### Base Information

- **PR Number:** #31572
- **Author:** [Tmn07](https://github.com/Tmn07)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-01 21:13:22
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31572/files) (1):**
  - `vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py`

### Summary

**What changed and why**  
The PR fixes a type checking bug where `isinstance(kernel_type, MarlinLinearKernel)` was incorrectly used to compare a class reference. The condition should check if `kernel_type` is the `MarlinLinearKernel` class itself, not an instance of it, to properly enable activation quantization for W4A16 compressed-tensor models.

**Technical impact**  
This correction ensures that when using Marlin kernels with compressed-tensor W4A16 models, the activation data type (int8 or fp8) specified via the `VLLM_MARLIN_INPUT_DTYPE` environment variable is correctly applied. Without this fix, activation quantization would remain disabled for these models, potentially affecting performance and memory usage.

**Potential risks**  
The change is minimal and targeted, but there's a risk if `kernel_type` could ever be a subclass of `MarlinLinearKernel` instead of the exact class. In that case, the strict `is` check might fail where the previous `isinstance` check would have succeeded, though this seems unlikely given the context.

**Key insights**  
Always use `is` for comparing class identities and `isinstance()` for checking instance relationships. This fix is critical for users of W4A16 quantized models who rely on activation quantization for optimal performance. Developers should verify that no other similar type comparison issues exist in the codebase.

---

## 5. [[ROCm][CI] Fix ModernBERT token classification test](https://github.com/vllm-project/vllm/pull/31612)


### Base Information

- **PR Number:** #31612
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-01 20:19:08
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31612/files) (1):**
  - `tests/models/language/pooling/test_token_classification.py`

### Summary

**What changed and why**  
The PR fixes a test failure in ModernBERT token classification on ROCm by forcing HuggingFace to use eager attention instead of flash attention, which has known numerical accuracy issues on ROCm. Additionally, tensor conversion warnings are resolved by replacing `torch.tensor()` with `detach().clone()` as recommended by PyTorch.

**Technical impact**  
This change only affects ROCm platforms, ensuring HuggingFace inference produces results consistent with vLLM's FlexAttention backend. CUDA behavior remains unchanged, and the fix is isolated to the test’s HuggingFace runner, not vLLM’s core implementation.

**Potential risks**  
Using eager attention may increase inference latency and memory usage during testing on ROCm, though this is acceptable for a test environment. The fix is platform-specific, so any future changes to the `current_platform.is_rocm()` logic or HuggingFace’s attention implementations could require updates.

**Key insights**  
This is a workaround for a HuggingFace issue, not a vLLM bug, highlighting the importance of platform-specific testing. Developers should monitor upstream fixes in HuggingFace and consider removing this override once the flash attention accuracy issue is resolved. The tensor conversion fix improves code quality and should be applied consistently across similar test files.

---

## 6. [[Model] Enable LoRA support for tower and connector in LLaVA](https://github.com/vllm-project/vllm/pull/31513)


### Base Information

- **PR Number:** #31513
- **Author:** [jayhemnani9910](https://github.com/jayhemnani9910)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-01 19:32:39
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31513/files) (2):**
  - `docs/models/supported_models.md`
  - `vllm/model_executor/models/llava.py`

### Summary

**What changed and why**  
This PR adds LoRA (Low-Rank Adaptation) support to LLaVA models by implementing the required interface methods. The changes enable LoRA adapters to be applied to the vision tower and connector components, allowing fine-tuning of multimodal capabilities. The implementation leverages existing infrastructure by adding `SupportsLoRA` to the class inheritance and providing token mapping methods.

**Technical impact**  
The modifications extend `LlavaForConditionalGeneration` to support the `SupportsLoRA` interface, enabling targeted parameter-efficient fine-tuning of vision components. Since LLaVA uses a simple MLP projector that maintains a 1:1 token mapping, the token helper methods return input values unchanged. This change also automatically provides LoRA support to `MantisForConditionalGeneration` through inheritance.

**Potential risks**  
The 1:1 token mapping assumption may not hold for future LLaVA variants with different projector architectures. There's a risk of incompatibility if the vision tower outputs tokens differently than expected. The changes also introduce new public methods that must maintain backward compatibility in future updates.

**Key insights**  
Developers can now apply LoRA adapters specifically to LLaVA's vision components for efficient fine-tuning. The implementation correctly handles LLaVA's architecture where vision features maintain token count through projection. When extending this to other multimodal models, verify the token mapping logic matches the actual architecture.

---

## 7. [[Bugfix] Fix block size used in EAGLE slot mapping](https://github.com/vllm-project/vllm/pull/31540)


### Base Information

- **PR Number:** #31540
- **Author:** [benchislett](https://github.com/benchislett)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-01 19:32:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31540/files) (1):**
  - `vllm/v1/spec_decode/eagle.py`

### Summary

**What changed and why**  
The fix addresses a bug where EAGLE slot mapping incorrectly used a global block size from `vllm_config.cache_config.block_size`. For hybrid models with different block sizes for linear and attention layers, this caused incorrect slot calculations, leading to crashes in long-context or high-concurrency scenarios. The solution dynamically retrieves the correct block size from the KV cache specification of the specific attention metadata builder being used.

**Technical impact**  
This change ensures slot mapping calculations align with the actual KV cache block size for each attention operation, which is critical for models with heterogeneous block configurations. It eliminates subtle slot mapping errors that could corrupt KV cache updates or cause out-of-bounds accesses, particularly when sequences approach the maximum model length or under high load.

**Potential risks**  
If the `kv_cache_spec.block_size` varies unexpectedly between different attention metadata builders within the same operation, it could introduce inconsistencies. There's also a risk if the `kv_cache_spec` is not properly initialized or accessible when slot mapping is computed, which might lead to runtime errors.

**Key insights**  
Always use the block size from the specific KV cache group involved in the operation rather than a global configuration. Developers should verify that `attn_metadata_builder.kv_cache_spec` and `tree_attn_metadata_builder.kv_cache_spec` are consistently available and correctly populated in all execution paths to prevent regressions.

---

## 8. [feat: support LoRA for DeepSeek-OCR(Language Model part)](https://github.com/vllm-project/vllm/pull/31569)


### Base Information

- **PR Number:** #31569
- **Author:** [zhima771](https://github.com/zhima771)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-01 19:32:11
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31569/files) (2):**
  - `docs/models/supported_models.md`
  - `vllm/model_executor/models/deepseek_ocr.py`

### Summary

**What changed and why**  
This PR adds LoRA (Low-Rank Adaptation) support to the DeepSeek-OCR model's language model component. The changes involve updating the model class to inherit from the `SupportsLoRA` interface and implementing the required `get_mm_mapping` method to define module prefixes for multimodal components. The documentation is also updated to reflect LoRA support.

**Technical impact**  
The DeepSeek-OCR model now supports fine-tuning via LoRA adapters, enabling efficient parameter-efficient tuning without full model retraining. This integrates the model into vLLM's existing LoRA infrastructure, allowing dynamic adapter loading and inference as demonstrated in the test plan.

**Potential risks**  
If the `get_mm_mapping` method incorrectly defines module prefixes (e.g., mismatched keys), it could lead to failed weight loading or runtime errors. Additionally, the LoRA rank (`--max-lora-rank=64`) must align with the adapter's configuration to avoid compatibility issues.

**Key insights**  
The implementation correctly follows vLLM's pattern for adding LoRA support to multimodal models. Developers should ensure that any future changes to the model's architecture (e.g., module names) are reflected in the `MultiModelKeys` mapping. The successful test with a real adapter confirms functional integration.

---

## 9. [[ROCm][CI] Fix failure in Language Models Tests (Extra Standard) by reducing agent pool size](https://github.com/vllm-project/vllm/pull/31553)


### Base Information

- **PR Number:** #31553
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-01 19:29:42
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31553/files) (1):**
  - `.buildkite/test-amd.yaml`

### Summary

**What changed and why**  
The change switches the CI job's agent pool from `mi325_8` (8 GPUs) to `mi325_2` (2 GPUs) to align with the job's `parallelism: 2` setting. This prevents the `pytest-shard` plugin from creating empty shards when distributing a small number of tests (~19) across an excessive number of workers, which was causing job failures with exit code 5.

**Technical impact**  
This fix ensures the test sharding logic matches the available parallel execution slots, guaranteeing each shard receives at least one test. It also adds `TORCH_NCCL_BLOCKING_WAIT=1` to the environment, which may aid in debugging distributed operations but is not directly related to the sharding issue.

**Potential risks**  
If the number of qualifying tests (`core_model and slow_test`) decreases further, the same empty-shard issue could reoccur even with two shards. The added NCCL environment variable could introduce performance overhead or alter the behavior of collective operations during tests.

**Key insights**  
Always align test parallelization settings with the actual hardware resources to avoid empty shards in pytest. Consider implementing a check in the CI script to skip sharding or adjust the shard count dynamically when the test count is very low. The NCCL variable addition should be evaluated for necessity, as it might be a separate debugging change.

---

## 10. [[Bugfix] Replace BaseException with specific exceptions in FLA utils](https://github.com/vllm-project/vllm/pull/31590)


### Base Information

- **PR Number:** #31590
- **Author:** [c0de128](https://github.com/c0de128)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-01 19:27:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31590/files) (1):**
  - `vllm/model_executor/layers/fla/ops/utils.py`

### Summary

**What changed and why**  
The change replaces a broad `except BaseException:` with a targeted `except (RuntimeError, AttributeError):` in the `get_available_device()` function. This ensures that only expected exceptions from Triton backend access (like `RuntimeError` for unavailable backends or `AttributeError` for missing attributes) are caught, while allowing critical system exceptions such as `SystemExit` and `KeyboardInterrupt` to propagate normally.

**Technical impact**  
This modification refines the error-handling behavior of device detection in FLA utilities. By narrowing the exception scope, the function now properly handles Triton-specific failures without inadvertently suppressing signals or termination requests that should be managed at a higher level, improving the robustness of exception propagation.

**Potential risks**  
If Triton introduces new exception types for backend or attribute access failures, they may no longer be caught, potentially causing unhandled exceptions. Additionally, any existing code relying on the previous behavior where all exceptions (including system-level ones) were silently caught and defaulted to "cpu" might see different behavior, though this is intentional.

**Key insights**  
The change aligns with best practices for exception handling by avoiding overly broad catch-all clauses. Developers should ensure that any new Triton-related exceptions are reviewed and added to the tuple if necessary. This also highlights the importance of allowing system-critical exceptions to propagate to maintain proper application lifecycle management.

---

## 11. [Add Multimodal Processor Benchmark](https://github.com/vllm-project/vllm/pull/29105)


### Base Information

- **PR Number:** #29105
- **Author:** [reaganjlee](https://github.com/reaganjlee)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-01 19:26:54
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29105/files) (13):**
  - `docs/cli/bench/mm_processor.md`
  - `docs/mkdocs/hooks/generate_argparse.py`
  - `vllm/benchmarks/datasets.py`
  - `vllm/benchmarks/mm_processor.py`
  - `vllm/benchmarks/throughput.py`
  - `vllm/config/observability.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/entrypoints/cli/__init__.py`
  - `vllm/entrypoints/cli/benchmark/mm_processor.py`
  - `vllm/inputs/preprocess.py`
  - `vllm/multimodal/processing.py`
  - `vllm/multimodal/registry.py`
  - `vllm/v1/engine/input_processor.py`

### Summary

**What changed and why**  
This PR adds a new benchmark for the multimodal processor, addressing issue #24171. It introduces a `vllm bench mm-processor` command that measures latency and timing statistics for multimodal processing on a single GPU instance. The implementation includes CLI integration, dataset handling extensions for multimodal data, and detailed timing collection across processing stages (HF processor, hashing, cache lookup, prompt update).

**Technical impact**  
The changes extend the benchmarking framework to support multimodal workloads, adding new dataset options (`random-mm`, `random-rerank`) and refactoring dataset argument handling for better modularity. It introduces observability configurations (`enable_mm_processor_stats`) and thread-safe timing collection in the multimodal processing pipeline. The benchmark outputs structured metrics (mean, median, percentiles) for both per-stage processing times and end-to-end latency.

**Potential risks**  
- The timing stats registry may leak memory if not cleared properly after benchmark runs, especially in long-running services.  
- Concurrent access to the timing registry could cause contention in high-load scenarios, though a lock is provided.  
- The benchmark assumes multimodal stats are always available; missing stats (e.g., due to registry clearing) may produce misleading warnings.  
- Argument precedence between `--input-len` and `--random-input-len` (and similar pairs) could cause confusion if both are specified.

**Key insights**  
- The PR modularizes dataset argument parsing, promoting reuse across benchmarks.  
- Timing instrumentation is non-invasive and conditionally enabled via config, minimizing overhead in production.  
- Developers should ensure the timing registry is cleared after stats retrieval to prevent memory buildup.  
- Consider adding validation to ensure multimodal models are used when running this benchmark, as non-multimodal models will yield empty stats.

---

