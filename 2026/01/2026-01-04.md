# vLLM Merged PR Report

**Report Date:** 2026-01-04 PST

**Total Merged PRs:** 16

---

## 1. [[ROCM] Reorder arguments and rename parameters for rope_cached_thd_positions_2c_fwd_inplace](https://github.com/vllm-project/vllm/pull/29993)


### Base Information

- **PR Number:** #29993
- **Author:** [tpopp](https://github.com/tpopp)
- **Merged By:** [tjtanaa](https://github.com/tjtanaa)
- **Merged time:** 2026-01-04 23:37:57
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29993/files) (1):**
  - `vllm/_aiter_ops.py`

### Summary

**What changed and why**  
The function call to `rope_cached_thd_positions_2c_fwd_inplace` was updated to reorder arguments and rename the `is_nope_first` parameter to `nope_first`. This change aligns the call signature with the function's actual parameter list in the ROCm/AIter codebase, fixing a `TypeError` caused by argument mismatches.

**Technical impact**  
This correction ensures the Triton kernel for rotary positional embeddings can be properly invoked when the `VLLM_ROCM_USE_AITER_TRITON_ROPE` flag is enabled. It resolves runtime errors and allows the kernel to execute, as evidenced by the successful lm_eval test results after the change.

**Potential risks**  
If other parts of the codebase call this function with the old signature, they may break. Additionally, the argument reordering (moving `positions` later in the list) could introduce subtle bugs if the positional dependencies of `sin`, `cos`, `query_`, and `key_` are not fully understood or documented.

**Key insights**  
Always verify that function call signatures match their definitions, especially when integrating external libraries like ROCm/AIter. The test results show comparable performance between the old and new paths, indicating the fix is functionally correct. Developers should ensure consistent parameter naming across all call sites to prevent similar issues.

---

## 2. [[CI] Bump sentence-transformer from 3.2.1 to 5.2.0](https://github.com/vllm-project/vllm/pull/31664)


### Base Information

- **PR Number:** #31664
- **Author:** [noooop](https://github.com/noooop)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-04 21:45:01
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31664/files) (3):**
  - `requirements/nightly_torch_test.txt`
  - `requirements/test.in`
  - `requirements/test.txt`

### Summary

**What changed and why**  
The PR updates the `sentence-transformers` dependency from version 3.2.1 to 5.2.0 across three requirement files. This change resolves a CI test error that only occurred with the outdated version 3.2.1, as upgrading is preferable to manually patching the old version.

**Technical impact**  
Upgrading to a much newer major version (3.2.1 → 5.2.0) may introduce API changes or behavioral differences in the library, but since it's only used for CI testing and model pooling, the impact should be limited to test environments. The dependency pin is now explicit (`>=5.2.0`) in the `.in` files, while `test.txt` locks it to exactly 5.2.0.

**Potential risks**  
A major version jump could break compatibility if the library's API has changed significantly, though the provided test results indicate passing tests. There is also a risk of transitive dependency conflicts, as the update may pull in newer versions of other packages (e.g., `typing-extensions` now appears as a dependency of `sentence-transformers` in `test.txt`).

**Key insights**  
This is a proactive maintenance update that eliminates a known CI failure by replacing an outdated dependency. Developers should verify that any local testing or development environments align with the new version to avoid discrepancies. The explicit version constraint (`>=5.2.0`) helps prevent future regressions to older, problematic versions.

---

## 3. [[Frontend] [Bugfix] respect server-level default chat template kwargs in reasoning parser](https://github.com/vllm-project/vllm/pull/31581)


### Base Information

- **PR Number:** #31581
- **Author:** [cjackal](https://github.com/cjackal)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-01-04 21:42:47
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31581/files) (2):**
  - `vllm/entrypoints/openai/serving_chat.py`
  - `vllm/entrypoints/openai/serving_engine.py`

### Summary

**What changed and why**  
The PR fixes a bug where server-level default chat template keyword arguments (like `enable_thinking`) were not being passed to the reasoning parser. Previously, these defaults were only applied during tokenization, causing a mismatch for hybrid reasoning models that rely on these kwargs to select the correct parser. The changes ensure the same merged kwargs are used in both tokenization and reasoning parsing.

**Technical impact**  
This change ensures consistency between the chat template rendering and reasoning parsing subsystems. It introduces a helper method `_prepare_extra_chat_template_kwargs` to centralize the merging of server defaults and request-specific kwargs, which is now called from both the tokenization logic and the reasoning parser initialization paths.

**Potential risks**  
If the merging order (server defaults overridden by request kwargs) differs from other parts of the system, it could cause subtle inconsistencies. The PR author also notes that the responses API doesn't reflect chat template kwargs at all, which could lead to incomplete fixes for related use cases.

**Key insights**  
The fix correctly addresses the core issue by ensuring reasoning parsers receive the same merged kwargs as tokenizers. Developers should be aware of the broader issue with the responses API mentioned in the PR description, as it may require follow-up work. The centralized helper method improves maintainability and reduces duplication.

---

## 4. [[Bugfix] Fix EPLB state logging error](https://github.com/vllm-project/vllm/pull/31455)


### Base Information

- **PR Number:** #31455
- **Author:** [tlrmchlsmth](https://github.com/tlrmchlsmth)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-01-04 20:06:28
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31455/files) (1):**
  - `vllm/distributed/eplb/eplb_state.py`

### Summary

**What changed and why**  
The change fixes a logging format string error in the EPLB state module. The original code used `%d` (integer format specifier) to log `policy_type`, but the actual value is a string, causing a `TypeError`. The fix changes the format specifier to `%s` to correctly handle string values.

**Technical impact**  
This is a minimal change that resolves a runtime logging error without affecting system functionality. It ensures debug logging works correctly when EPLB policies are selected, preventing unnecessary tracebacks in logs while maintaining the same informational output.

**Potential risks**  
The risk is extremely low since this only affects debug logging. However, if `policy_type` could ever be a non-string type (like an enum or integer), the new format might mask type inconsistencies that should be addressed elsewhere in the code.

**Key insights**  
Always verify format specifiers match the actual variable types, especially in logging statements. Consider adding type hints or assertions for `policy_type` if its data type contract isn't clear. This fix highlights the importance of consistent typing in configuration parameters.

---

## 5. [[log] enable max_log_len trim only when needed](https://github.com/vllm-project/vllm/pull/31482)


### Base Information

- **PR Number:** #31482
- **Author:** [andyxning](https://github.com/andyxning)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-04 19:55:43
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31482/files) (1):**
  - `vllm/entrypoints/logger.py`

### Summary

**What changed and why**  
The change wraps the prompt/prompt_token_ids truncation logic and DEBUG-level logging inside a conditional check for `logger.isEnabledFor(logging.DEBUG)`. This ensures that string slicing and debug logging only occur when DEBUG logging is actually enabled, avoiding unnecessary CPU work in production environments where DEBUG is typically disabled.

**Technical impact**  
This optimization reduces latency for serving generate requests by eliminating string manipulation and debug formatting overhead in the critical path when DEBUG logging is off. The INFO-level log remains unconditional, preserving essential request tracking. The change maintains backward compatibility—debug behavior is unchanged when DEBUG is enabled.

**Potential risks**  
If DEBUG logging is enabled, the conditional check adds a small overhead (negligible compared to the trimming work). There’s a risk that developers might assume the truncation always applies, but it now depends on the logging level. No functional regression is expected, as the debug output remains identical when DEBUG is active.

**Key insights**  
This is a performance-focused change that aligns computational cost with actual logging needs. Developers should note that prompt truncation for debug purposes is now tied to the DEBUG log level. Consider applying similar patterns elsewhere in the codebase where debug-only processing occurs in hot paths. Ensure logging configuration in production keeps DEBUG disabled to realize the latency benefit.

---

## 6. [Add chat prefix completion feature to DeepSeek v3.2](https://github.com/vllm-project/vllm/pull/31147)


### Base Information

- **PR Number:** #31147
- **Author:** [PHOEBEMOON0802](https://github.com/PHOEBEMOON0802)
- **Merged By:** [chaunceyjiang](https://github.com/chaunceyjiang)
- **Merged time:** 2026-01-04 19:20:25
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31147/files) (1):**
  - `vllm/tokenizers/deepseek_v32_encoding.py`

### Summary

**What changed and why**  
Added support for prefix continuation in DeepSeek v3.2 by introducing an `is_prefix` flag that modifies how assistant messages are formatted. When `is_prefix` is true and there are no tool calls, the system now appends only the summary content without the full assistant template wrapper.

**Technical impact**  
This change enables partial message completion scenarios where the model can continue from a prefix without the structural overhead of the standard assistant message format. The modification is backward compatible since it only affects behavior when the new `prefix` field is explicitly set to True in the message dictionary.

**Potential risks**  
If `is_prefix=True` is incorrectly set when tool calls are present, the tool calls content will be omitted from the prompt, potentially breaking tool invocation functionality. There's also a risk that callers might misuse this flag for non-assistant roles, though the current implementation would raise `NotImplementedError`.

**Key insights**  
The implementation correctly isolates the prefix behavior to assistant messages without tool calls, maintaining existing functionality for other cases. Developers should ensure proper validation when setting the `prefix` flag and document that it's only meaningful for assistant messages in non-tool-call contexts. Consider adding explicit validation to prevent misuse with tool calls.

---

## 7. [[CI Failure] Fix NomicBert max_model_len validation](https://github.com/vllm-project/vllm/pull/31662)


### Base Information

- **PR Number:** #31662
- **Author:** [noooop](https://github.com/noooop)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-01-04 19:06:52
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31662/files) (2):**
  - `vllm/config/vllm.py`
  - `vllm/model_executor/models/config.py`

### Summary

**What changed and why**  
The PR removes the `recalculate_max_model_len` method from `VllmConfig` and refactors NomicBert's max_model_len validation to directly use `ModelConfig.get_and_verify_max_len()`. This eliminates a method that was only used for NomicBert validation and prevents potential misuse, while ensuring consistent validation logic.

**Technical impact**  
The changes centralize max_model_len validation within the `ModelConfig` class, improving consistency and reducing coupling between `VllmConfig` and model-specific logic. The NomicBert validation now properly updates `model_arch_config` fields and handles context extension scenarios more directly.

**Potential risks**  
Removing the dedicated method could affect other models if they were using it (though the PR states it was only for NomicBert). The validation logic now relies on `ModelConfig.get_and_verify_max_len()` which must handle all edge cases previously managed by the removed method. The addition of `derived_max_model_len_and_key` caching needs to be thread-safe.

**Key insights**  
This is a good cleanup that reduces API surface area and consolidates validation logic. Developers should ensure no other code paths depend on `recalculate_max_model_len`. The pattern of moving model-specific validation into `ModelConfig` methods improves separation of concerns and should be considered for similar refactoring opportunities.

---

## 8. [[Misc] Various code simplifications](https://github.com/vllm-project/vllm/pull/31666)


### Base Information

- **PR Number:** #31666
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-04 18:35:56
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31666/files) (7):**
  - `vllm/v1/core/sched/async_scheduler.py`
  - `vllm/v1/core/sched/interface.py`
  - `vllm/v1/core/sched/scheduler.py`
  - `vllm/v1/spec_decode/eagle.py`
  - `vllm/v1/structured_output/__init__.py`
  - `vllm/v1/structured_output/utils.py`
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR consists of various code simplifications and cleanups, primarily focused on formatting and minor logic improvements. Changes include removing unnecessary line breaks in multi-line function signatures, simplifying conditional logic, improving variable naming, and optimizing imports. The goal is to enhance code readability and maintain consistency across the codebase.

**Technical impact**  
The changes are largely cosmetic and do not alter core functionality. However, some logic simplifications—such as the refactoring in `structured_output/__init__.py` where token iteration now uses a sentinel value (`-1`) instead of `None`—may slightly affect performance or clarity. The overall architecture and system behavior remain unchanged.

**Potential risks**  
- The sentinel value change (`-1` replacing `None`) in `structured_output/__init__.py` could introduce subtle bugs if not handled consistently elsewhere.  
- Simplified conditionals (e.g., removing redundant checks) might overlook edge cases, though the logic appears equivalent.  
- Formatting changes, while benign, could cause merge conflicts in active development branches.

**Key insights**  
- The PR improves code consistency by standardizing multi-line function signatures and reducing verbosity.  
- Developers should verify that the sentinel value `-1` is universally recognized as a padding token in the structured output logic.  
- These changes are low-risk but warrant careful review to ensure no functional regressions, especially in the grammar bitmask and token processing logic.

---

## 9. [[Platform] Deprecate seed_everything](https://github.com/vllm-project/vllm/pull/31659)


### Base Information

- **PR Number:** #31659
- **Author:** [wangxiyuan](https://github.com/wangxiyuan)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-04 18:34:04
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31659/files) (77):**
  - `docs/design/plugin_system.md`
  - `tests/compile/distributed/test_async_tp.py`
  - `tests/compile/distributed/test_fusion_all_reduce.py`
  - `tests/compile/distributed/test_sequence_parallelism.py`
  - `tests/kernels/attention/test_aiter_flash_attn.py`
  - `tests/kernels/attention/test_attention.py`
  - `tests/kernels/attention/test_cache.py`
  - `tests/kernels/attention/test_cascade_flash_attn.py`
  - `tests/kernels/attention/test_cpu_attn.py`
  - `tests/kernels/attention/test_flash_attn.py`
  - `tests/kernels/attention/test_flashinfer.py`
  - `tests/kernels/attention/test_flashinfer_trtllm_attention.py`
  - `tests/kernels/attention/test_lightning_attn.py`
  - `tests/kernels/attention/test_mha_attn.py`
  - `tests/kernels/attention/test_prefix_prefill.py`
  - `tests/kernels/attention/test_triton_unified_attention.py`
  - `tests/kernels/core/test_activation.py`
  - `tests/kernels/core/test_fused_qk_norm_rope.py`
  - `tests/kernels/core/test_layernorm.py`
  - `tests/kernels/core/test_mrope.py`
  - `tests/kernels/core/test_pos_encoding.py`
  - `tests/kernels/mamba/test_causal_conv1d.py`
  - `tests/kernels/mamba/test_mamba_mixer2.py`
  - `tests/kernels/mamba/test_mamba_ssm.py`
  - `tests/kernels/mamba/test_mamba_ssm_ssd.py`
  - `tests/kernels/moe/modular_kernel_tools/make_feature_matrix.py`
  - `tests/kernels/moe/modular_kernel_tools/profile_modular_kernel.py`
  - `tests/kernels/moe/test_batched_moe.py`
  - `tests/kernels/moe/test_cpu_fused_moe.py`
  - `tests/kernels/moe/test_cutlass_moe.py`
  - `tests/kernels/moe/test_deepep_deepgemm_moe.py`
  - `tests/kernels/moe/test_deepep_moe.py`
  - `tests/kernels/moe/test_flashinfer.py`
  - `tests/kernels/moe/test_flashinfer_moe.py`
  - `tests/kernels/moe/test_grouped_topk.py`
  - `tests/kernels/moe/test_modular_kernel_combinations.py`
  - `tests/kernels/moe/test_modular_oai_triton_moe.py`
  - `tests/kernels/moe/test_moe.py`
  - `tests/kernels/moe/test_moe_align_block_size.py`
  - `tests/kernels/moe/test_moe_permute_unpermute.py`
  - `tests/kernels/moe/test_nvfp4_moe.py`
  - `tests/kernels/moe/test_pplx_cutlass_moe.py`
  - `tests/kernels/moe/test_pplx_moe.py`
  - `tests/kernels/moe/test_silu_mul_fp8_quant_deep_gemm.py`
  - `tests/kernels/moe/test_silu_mul_per_token_group_quant_fp8_colmajor.py`
  - `tests/kernels/quantization/test_awq_triton.py`
  - `tests/kernels/quantization/test_cutlass_w4a8_moe.py`
  - `tests/kernels/quantization/test_flashinfer_nvfp4_scaled_mm.py`
  - `tests/kernels/quantization/test_flashinfer_scaled_mm.py`
  - `tests/kernels/quantization/test_fp8_quant.py`
  - `tests/kernels/quantization/test_fp8_quant_group.py`
  - `tests/kernels/quantization/test_gguf.py`
  - `tests/kernels/quantization/test_int8_quant.py`
  - `tests/kernels/quantization/test_mxfp4_qutlass.py`
  - `tests/kernels/quantization/test_nvfp4_quant.py`
  - `tests/kernels/quantization/test_nvfp4_qutlass.py`
  - `tests/kernels/quantization/test_nvfp4_scaled_mm.py`
  - `tests/kernels/quantization/test_silu_mul_nvfp4_quant.py`
  - `tests/kernels/quantization/test_triton_scaled_mm.py`
  - `tests/kernels/test_apply_repetition_penalties.py`
  - `tests/kernels/test_fla_layernorm_guard.py`
  - `tests/lora/test_fused_moe_lora_kernel.py`
  - `tests/lora/test_layers.py`
  - `tests/lora/test_punica_ops.py`
  - `tests/models/test_vision.py`
  - `tests/v1/attention/test_attention_backends.py`
  - `tests/v1/kv_offload/test_cpu_gpu.py`
  - `tests/v1/tpu/test_mha_attn.py`
  - `tests/v1/worker/test_gpu_model_runner.py`
  - `vllm/model_executor/__init__.py`
  - `vllm/model_executor/utils.py`
  - `vllm/platforms/interface.py`
  - `vllm/utils/torch_utils.py`
  - `vllm/v1/worker/cpu_worker.py`
  - `vllm/v1/worker/gpu_worker.py`
  - `vllm/v1/worker/tpu_worker.py`
  - `vllm/v1/worker/xpu_worker.py`

### Summary

**What changed and why**  
The PR deprecates the `seed_everything` platform interface because it's considered redundant—no platform redefines it, and it always performs the same operations (`random.seed(seed)`, `np.random.seed(seed)`, `torch.manual_seed(seed)`). The goal is to clean up the platform interface. All usages have been replaced with `vllm.utils.torch_utils.set_random_seed`.

**Technical impact**  
This change removes a redundant abstraction layer. The `seed_everything` method in the platform interface is marked as deprecated with a warning, and its implementation is moved to a standalone utility function. This simplifies the codebase by eliminating an unnecessary interface method and consolidates seeding logic into a single, direct function. The deprecation is documented, and a removal timeline (v0.14.0 or later) is provided.

**Potential risks**  
The main risk is that external code relying on the deprecated `current_platform.seed_everything` will break after removal. However, the deprecation warning and documentation update should mitigate this. There's also a slight risk of missing some usages in external or internal code not covered by the PR, but the extensive test updates suggest good coverage. The change could affect reproducibility if the new utility function behaves differently, but the implementation is identical.

**Key insights**  
Developers should immediately update any code using `current_platform.seed_everything` to use `vllm.utils.torch_utils.set_random_seed`. The deprecation is well-managed with clear documentation and a removal version. This change aligns with reducing unnecessary abstractions and simplifying the platform interface. Ensure all new code uses the new utility function to avoid future migration efforts.

---

## 10. [[CI/Build] Revive skipped reward models e2e test](https://github.com/vllm-project/vllm/pull/31665)


### Base Information

- **PR Number:** #31665
- **Author:** [Isotr0py](https://github.com/Isotr0py)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-04 18:33:46
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31665/files) (2):**
  - `tests/models/fixtures/qwen2_5_math_prm_reward_step.json`
  - `tests/models/language/pooling/test_reward.py`

### Summary

**What changed and why**  
This PR revives a previously skipped end-to-end test for reward models by adding a golden fixture-based test. The original test was broken after transformers v4.53, so the fix introduces a new test that compares vLLM outputs against pre-saved golden outputs, ensuring at least one working e2e test for the reward entrypoint.

**Technical impact**  
The changes add a fixture file for expected outputs and modify the test to run two variants: one that compares against Hugging Face outputs (still version-gated) and another that validates against golden outputs. This decouples the test from transformers version constraints and provides a stable regression test for reward model functionality.

**Potential risks**  
The golden fixture may become outdated if the model's behavior changes due to updates in vLLM or underlying dependencies. Additionally, the tolerance (1.5e-2) might be too permissive or too strict for certain hardware or precision variations, potentially causing flaky tests.

**Key insights**  
Developers should update the golden fixture if the reward model's expected outputs change. The dual-test approach balances compatibility testing (against Hugging Face) and stability (against fixtures). Ensure any new reward models added to the test suite have corresponding golden fixtures to maintain test coverage.

---

## 11. [[Minor] Small pooler output processing optimization](https://github.com/vllm-project/vllm/pull/31667)


### Base Information

- **PR Number:** #31667
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-04 18:33:12
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31667/files) (1):**
  - `vllm/v1/worker/gpu_model_runner.py`

### Summary

**What changed and why**  
This PR optimizes pooler output processing by moving the final output grouping/slicing operation before CPU synchronization. The change recognizes that since the slicing operation only creates views of existing tensors (not new allocations), it can be safely performed during the async copy phase rather than waiting for full synchronization.

**Technical impact**  
The optimization reduces latency by performing output filtering concurrently with asynchronous GPU-to-CPU transfers. This eliminates a sequential dependency where the system previously waited for full CPU sync before filtering outputs, potentially improving throughput for pooling operations. The architecture maintains the same output semantics while better utilizing GPU-CPU parallelism.

**Potential risks**  
If the assumption that slicing "only contains views" is incorrect for certain tensor types or configurations, this could lead to premature tensor deallocation or memory issues. The removal of `self._finished_mask` storage in `AsyncPoolerOutput` means the mask must be used immediately during construction, which could cause issues if the mask tensor lifecycle isn't properly managed.

**Key insights**  
This is a well-targeted optimization that leverages view semantics to improve pipeline parallelism. Developers should verify that all tensor operations in the slicing path truly create views rather than copies. Consider adding a comment documenting the view assumption for future maintainers, and ensure proper synchronization between the async copy stream and mask availability.

---

## 12. [[Bugfix] Fix  AttributeError: 'Stream' object has no attribute 'dp_size'](https://github.com/vllm-project/vllm/pull/31663)


### Base Information

- **PR Number:** #31663
- **Author:** [jeejeelee](https://github.com/jeejeelee)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-04 18:31:31
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31663/files) (1):**
  - `vllm/lora/layers/fused_moe.py`

### Summary

**What changed and why**  
A single line was removed from the `_inject_lora_into_fused_moe` method in `vllm/lora/layers/fused_moe.py`. The line passed `getattr(self.base_layer, "shared_experts_stream", None)` as an argument, but this attribute no longer exists after PR #31533, causing an `AttributeError` during LoRA MoE initialization.

**Technical impact**  
This change resolves a regression introduced by the prior deletion of the `shared_experts_stream` attribute. It ensures that LoRA initialization for MoE models works correctly by removing the reference to the nonexistent attribute, aligning the code with the updated `FusedMoEModularKernel` structure.

**Potential risks**  
If the removed argument was still required elsewhere or had functional significance, its removal might break other code paths. Additionally, there is a risk that similar references to `shared_experts_stream` could exist in other parts of the codebase, potentially causing hidden failures.

**Key insights**  
This is a straightforward fix for a clear regression, but developers should verify that no other dependencies on `shared_experts_stream` remain. It’s also advisable to add a test case for LoRA MoE initialization to catch such regressions early in future changes.

---

## 13. [[ROCm][CI] Fix language generation test accuracy by disabling HF flash_sdp and mem_efficient_sdp](https://github.com/vllm-project/vllm/pull/31597)


### Base Information

- **PR Number:** #31597
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-04 18:17:05
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31597/files) (1):**
  - `tests/models/language/generation/conftest.py`

### Summary

**What changed and why**  
A new `conftest.py` file was added to disable PyTorch's `flash_sdp` and `mem_efficient_sdp` backends and enable `math_sdp` specifically for ROCm platforms during test sessions. This change addresses numerical accuracy failures in language generation tests where HuggingFace Transformers outputs diverged from vLLM's outputs due to known ROCm-specific issues in the optimized attention implementations.

**Technical impact**  
This configuration ensures that HuggingFace Transformers uses the more accurate, reference `math_sdp` backend for scaled dot-product attention during inference on ROCm. This creates a stable numerical baseline for test comparisons, allowing the language generation tests to pass by aligning the attention computation path used by HuggingFace with vLLM's expected behavior.

**Potential risks**  
The workaround may introduce performance regression during testing since `math_sdp` is computationally less efficient than the optimized backends. There's also a risk that this platform-specific configuration could be overlooked or forgotten, creating technical debt. If the upstream ROCm issues are resolved, tests might continue using the slower backend unless this configuration is explicitly removed.

**Key insights**  
This is a targeted, temporary fix for a known upstream issue that should be clearly marked with a TODO comment for future removal. Developers should monitor the linked upstream issue (#30167) for resolution and plan to revert this configuration once ROCm's SDP implementations are numerically stable. The warning message provides good visibility into the active workaround during test execution.

---

## 14. [[CI] Skip Phi-MoE test due to old API util](https://github.com/vllm-project/vllm/pull/31632)


### Base Information

- **PR Number:** #31632
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [noooop](https://github.com/noooop)
- **Merged time:** 2026-01-04 16:52:07
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31632/files) (2):**
  - `.buildkite/test-amd.yaml`
  - `tests/models/language/generation/test_phimoe.py`

### Summary

**What changed and why**  
This PR addresses three distinct issues: skipping Phi-MoE tests due to an upstream `DynamicCache` attribute error, temporarily using a forked mamba-ssm version to fix ROCm 7.0+ compatibility, and correcting a NomicBert `max_model_len` validation regression caused by stale cached values.

**Technical impact**  
The changes prevent test failures on ROCm by disabling a problematic test and using a workaround for mamba-ssm. The NomicBert fix ensures configuration validation works correctly across platforms, maintaining consistency in model length restrictions.

**Potential risks**  
Skipping the Phi-MoE test masks a real issue that could affect production usage. The temporary mamba-ssm fork introduces a dependency on an unmerged external fix, which may cause instability if the fork diverges or is removed. The NomicBert fix, while correct, should be validated to ensure no other configuration caches have similar issues.

**Key insights**  
Developers should monitor the upstream Phi-MoE issue for resolution and re-enable the test promptly. The mamba-ssm workaround is a stopgap; plan to revert to the official package once the upstream PR is merged. Review other model configurations for similar caching bugs to prevent regressions.

---

## 15. [[BugFix] Async scheduling: handle model forward errors more cleanly](https://github.com/vllm-project/vllm/pull/31611)


### Base Information

- **PR Number:** #31611
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-04 11:04:37
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31611/files) (1):**
  - `vllm/v1/engine/core.py`

### Summary

**What changed and why**  
The PR fixes error handling in async scheduling when model execution fails. Previously, when `execute_model()` raised an exception, it was logged via a future callback, but the core loop would later fail with a misleading `AttributeError` because `sample_tokens()` would return `None`. The changes modify the batch queue to store the original execution future and raise the root cause exception directly, eliminating the need for the error callback.

**Technical impact**  
The batch queue now stores a tuple containing the sampling future, scheduler output, and the original execution future. This allows the core loop to access the original exception when `model_output` is `None`, providing clearer error propagation. The `_log_err_callback` method is removed as error handling is now centralized in `step_with_batch_queue`.

**Potential risks**  
If `exec_model_fut.result()` raises an exception other than the original model execution error (e.g., a cancellation error), it could mask the root cause. The `RuntimeError("unexpected error")` fallback may not provide sufficient context if `exec_model_fut` is in an unexpected state. Edge cases where `model_output` is `None` for reasons other than `execute_model()` failure could trigger incorrect error handling.

**Key insights**  
The fix improves error transparency by propagating the original exception directly. Developers should ensure that `exec_model_fut` always contains the relevant exception when `model_output` is `None`. Consider adding more specific error types or logging to distinguish between different failure modes. The removal of the callback simplifies the error handling flow but centralizes responsibility in the core loop.

---

## 16. [[misc] Sort uvicorn log level description according to verbosity](https://github.com/vllm-project/vllm/pull/31137)


### Base Information

- **PR Number:** #31137
- **Author:** [andyxning](https://github.com/andyxning)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-04 10:45:38
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31137/files) (1):**
  - `vllm/entrypoints/openai/cli_args.py`

### Summary

**What changed and why**  
The PR reorders the log level options for uvicorn in the CLI argument definition from ascending to descending verbosity (most severe to least severe). This aligns the displayed order with typical logging severity conventions, improving readability and user experience.

**Technical impact**  
This change is purely cosmetic and affects only the CLI help text or documentation generated from the Literal type hint. It does not alter any functional behavior, validation logic, or runtime configuration of the uvicorn logger.

**Potential risks**  
There is minimal risk since the change is superficial. However, if any downstream tooling parses the Literal values as strings (e.g., for automated documentation), the order change might affect readability but not correctness.

**Key insights**  
Developers should note that log level validation remains unchanged; the reordering simply follows standard logging severity hierarchy. This is a minor UX improvement that makes the CLI more intuitive. Ensure any related documentation or examples referencing the order are updated if consistency is required.

---

