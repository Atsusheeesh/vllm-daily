# vLLM Merged PR Report

**Report Date:** 2026-01-02 PST

**Total Merged PRs:** 10

---

## 1. [[CI][Bugfix] Fix token counting in chunked prefill compl test](https://github.com/vllm-project/vllm/pull/31630)


### Base Information

- **PR Number:** #31630
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-02 22:28:49
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31630/files) (1):**
  - `tests/entrypoints/openai/test_chunked_prompt.py`

### Summary

**What changed and why**  
The test was incorrectly counting tokens by incrementing `tokens_received` by 1 for each chunk, regardless of how many tokens were actually batched in the chunk. The fix now counts tokens based on the length of the `logprobs.tokens` list, which accurately reflects the number of tokens returned in each chunk.

**Technical impact**  
This change ensures the test correctly validates token usage accounting in chunked prefills, aligning the test's token counting logic with the actual behavior of the system where multiple tokens can be batched into a single response chunk.

**Potential risks**  
If `logprobs` or `logprobs.tokens` is `None` for any chunk, the new assertions will fail, potentially masking other issues. The test now assumes all chunks contain logprobs data, which may not hold if the API response format changes or if logprobs are disabled.

**Key insights**  
Always validate assumptions about data availability before using it—consider defensive checks for `logprobs` presence. This fix highlights the importance of aligning test logic with actual system behavior, especially for batched or streaming responses where token counts per chunk can vary.

---

## 2. [Improve HF qwen3_omni: preserve audio_sample_rate in kwargs restructuring](https://github.com/vllm-project/vllm/pull/29255)


### Base Information

- **PR Number:** #29255
- **Author:** [jeremyteboul](https://github.com/jeremyteboul)
- **Merged By:** [Isotr0py](https://github.com/Isotr0py)
- **Merged time:** 2026-01-02 20:31:09
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/29255/files) (3):**
  - `tests/models/multimodal/processing/test_qwen3_omni.py`
  - `tests/multimodal/test_processing.py`
  - `vllm/model_executor/models/qwen3_omni_moe_thinker.py`

### Summary

**What changed and why**  
The fix addresses a bug where `audio_sample_rate` was lost during kwargs restructuring in `Qwen3OmniMoeProcessor` for older transformers versions (<4.58.0). Previously, when `mm_kwargs` were split into `audio_kwargs` and `text_kwargs`, the top-level `audio_sample_rate` wasn't moved into `audio_kwargs`, causing audio processing to fail because the HuggingFace `WhisperFeatureExtractor` expects it there.

**Technical impact**  
The changes ensure `audio_sample_rate` is correctly preserved and placed into `audio_kwargs` after restructuring. Additionally, validation logic warns users if the provided sample rate mismatches the model's expected rate, preventing silent errors. The restructuring now only affects transformers versions <4.58.0, maintaining compatibility with newer versions.

**Potential risks**  
If the `audio_sample_rate` validation logic is too restrictive, it might ignore legitimate user-provided rates that differ from the expected rate, potentially affecting audio quality. The warning could be overlooked in logs, leading to confusion. Edge cases where `audio_sample_rate` is `None` or missing are handled, but any future changes to the kwargs structure could reintroduce similar issues.

**Key insights**  
Developers should note that the fix is version-dependent and only applies to transformers <4.58.0. The added validation ensures alignment with the model's expected sampling rate, but warnings should be monitored. Comprehensive unit tests cover various sample rates and edge cases, providing a solid safety net for future changes.

---

## 3. [[Core] Parse vLLM engine required fields from hf_config to model_arch_config](https://github.com/vllm-project/vllm/pull/28454)


### Base Information

- **PR Number:** #28454
- **Author:** [charlotte12l](https://github.com/charlotte12l)
- **Merged By:** [heheda12345](https://github.com/heheda12345)
- **Merged time:** 2026-01-02 15:13:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/28454/files) (11):**
  - `tests/config/base_model_arch_groundtruth.json`
  - `tests/config/draft_model_arch_groundtruth.json`
  - `tests/config/test_model_arch_config.py`
  - `tests/models/utils.py`
  - `tests/test_config.py`
  - `tests/v1/metrics/test_perf_metrics.py`
  - `vllm/config/model.py`
  - `vllm/config/model_arch.py`
  - `vllm/config/speculative.py`
  - `vllm/config/vllm.py`
  - `vllm/transformers_utils/model_arch_config_convertor.py`

### Summary

**What changed and why**  
This PR introduces a new `ModelArchitectureConfig` class and associated converters to standardize vLLM runtime fields parsed from Hugging Face configurations. The changes centralize configuration parsing logic previously scattered in `model.py`, moving toward a cleaner architecture where model-specific normalization is handled by dedicated converters. Llama3 serves as the prototype implementation.

**Technical impact**  
The refactor decouples configuration parsing from the main `ModelConfig` class, improving modularity and maintainability. A new `model_arch_config` attribute now provides standardized fields (e.g., `hidden_size`, `head_size`, `num_experts`) used throughout the runtime. This sets the stage for migrating all model-specific normalization out of `model.py` and enables thinner, more consistent parsers leveraging Hugging Face’s `PretrainedConfig` normalization.

**Potential risks**  
- The migration is incomplete; several TODOs remain (e.g., Mistral parser, `max_model_len` normalization, full model migration).  
- Edge cases may arise for models not yet in the converter registry, though the fallback to `AutoConfig.from_pretrained` mitigates this.  
- Changes to quantization config handling and deepseek MLA detection could affect specialized models if not thoroughly tested.

**Key insights**  
- Developers should adopt the new `model_arch_config` for accessing standardized model attributes instead of directly querying `hf_config`.  
- Future work must complete the migration for all models and ensure clear separation between `model_info` and `model_arch_config` as noted in the linked issue.  
- The added comprehensive test suite provides a solid foundation for validating conversions across diverse model architectures.

---

## 4. [[Benchmark] Fix OOM during MoE kernel tuning for large models](https://github.com/vllm-project/vllm/pull/31604)


### Base Information

- **PR Number:** #31604
- **Author:** [massif-01](https://github.com/massif-01)
- **Merged By:** [robertgshaw2-redhat](https://github.com/robertgshaw2-redhat)
- **Merged time:** 2026-01-02 14:24:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31604/files) (1):**
  - `benchmarks/kernels/benchmark_moe.py`

### Summary

**What changed and why**  
Added periodic Triton JIT cache clearing during MoE kernel tuning to prevent out-of-memory (OOM) errors when tuning large models with many experts. The solution introduces a `clear_triton_cache()` function that clears Python garbage collection, CUDA memory, and Triton's JIT cache, called every 50 configurations (configurable via environment variable).

**Technical impact**  
This change reduces memory accumulation during kernel tuning by preventing Triton from caching compiled kernels indefinitely. It enables successful tuning of large MoE models (e.g., 128 experts) that previously failed due to memory growth from ~40GB to 100GB+, allowing the benchmark to complete across batch sizes 1-64.

**Potential risks**  
The cache clearing interval (default 50) may need tuning for different hardware configurations - too frequent clearing could impact performance, while too infrequent may not prevent OOM. The Triton cache clearing uses defensive programming with try/except blocks, but different Triton versions might have incompatible APIs. Disabling cache clearing (interval=0) could still cause OOM in memory-constrained environments.

**Key insights**  
The implementation demonstrates good defensive programming with environment variable configurability and graceful fallbacks when Triton isn't available. Developers should monitor memory usage when tuning different model sizes and adjust `VLLM_MOE_TUNE_CACHE_CLEAR_INTERVAL` accordingly. Consider making the interval dynamically adaptive based on actual memory usage rather than fixed configuration.

---

## 5. [[MoE Refactor] Explicit construct mk for flashinfer bf16 kernel](https://github.com/vllm-project/vllm/pull/31504)


### Base Information

- **PR Number:** #31504
- **Author:** [zyongye](https://github.com/zyongye)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-02 13:54:51
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31504/files) (1):**
  - `vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py`

### Summary

**What changed and why**  
The PR refactors FlashInfer MoE kernel integration to explicitly construct modular kernels instead of using partial functions. This aligns FlashInfer with the existing `self.kernel` calling convention used by other MoE implementations, improving code consistency and maintainability.

**Technical impact**  
FlashInfer experts are now instantiated as a `FlashInferExperts` class within the modular kernel framework, replacing the previous `partial` function approach. The weight-swapping logic for FlashInfer is moved from `process_weights_after_loading` to the kernel initialization phase, and a new `use_inplace` flag controls in-place operations based on the active kernel.

**Potential risks**  
The slight performance regression observed in GSM8K benchmarks (exact_match dropped from 0.8620 to 0.8506) suggests potential numerical differences or optimization impacts. The weight-swapping logic (`swap_w13_to_w31`) now occurs conditionally during kernel setup, which could affect weight tensor layouts if not handled consistently across different execution paths.

**Key insights**  
This refactor improves architectural consistency but requires validation of the performance impact. Developers should monitor benchmark results closely and ensure the weight-swapping logic is correctly applied for all FlashInfer configurations. The conditional `use_inplace` flag introduces a new state variable that must be managed correctly across different kernel types.

---

## 6. [[MoE Refactor] Split `invoke_fused_moe_kernel`](https://github.com/vllm-project/vllm/pull/31050)


### Base Information

- **PR Number:** #31050
- **Author:** [zyongye](https://github.com/zyongye)
- **Merged By:** [vllm-bot](https://github.com/vllm-bot)
- **Merged time:** 2026-01-02 13:47:16
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31050/files) (1):**
  - `vllm/model_executor/layers/fused_moe/fused_moe.py`

### Summary

**What changed and why**  
The PR splits the monolithic `invoke_fused_moe_kernel` function into three specialized kernel-invocation functions: `invoke_fused_moe_wna16_cuda_kernel`, `invoke_fused_moe_wna16_triton_kernel`, and `invoke_fused_moe_triton_kernel`. A new `dispatch_fused_moe_kernel` function now orchestrates which kernel to call based on quantization settings and hardware support. This refactor isolates the WNA16 kernels (both CUDA and Triton) to facilitate their future removal when support for SM70 GPUs is dropped, as newer Marlin kernels already provide integer quantization support from SM75 onward.

**Technical impact**  
This change improves code modularity by separating kernel dispatch logic from kernel implementation details. The architecture now clearly distinguishes between WNA16 kernels (for integer quantization) and other fused MoE kernels (for BF16, FP8, and other quantization types). The dispatch function centralizes the decision-making process, making it easier to maintain and extend. Performance should remain unchanged as the underlying kernel implementations are unmodified.

**Potential risks**  
The refactor introduces new function signatures and dependencies (e.g., `get_moe_wna16_block_config`), which could lead to integration issues if callers are not updated accordingly. There is also a risk of regression if the dispatch logic incorrectly selects kernels, especially around the conditions for using WNA16 CUDA vs. Triton kernels. The assertions added in the new functions (e.g., `assert block_shape is None or block_shape[0] == 0`) may fail for unexpected input shapes.

**Key insights**  
Developers should note that the WNA16 kernels are now explicitly marked for future removal (see the code comments). When updating callers, ensure they use `dispatch_fused_moe_kernel` instead of the old monolithic function. Pay close attention to the `block_shape` parameter and quantization flags, as these drive the kernel selection. This refactor sets the stage for cleaner deprecation of legacy GPU support.

---

## 7. [[MoE] Fix output_shape calculation in Attention layer to handle 3D query inputs](https://github.com/vllm-project/vllm/pull/31596)


### Base Information

- **PR Number:** #31596
- **Author:** [AndreasKaratzas](https://github.com/AndreasKaratzas)
- **Merged By:** [LucasWilkinson](https://github.com/LucasWilkinson)
- **Merged time:** 2026-01-02 07:46:23
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31596/files) (2):**
  - `vllm/attention/layer.py`
  - `vllm/model_executor/layers/quantization/fp8.py`

### Summary

**What changed and why**  
The PR fixes a regression where the Attention layer's `output_shape` calculation incorrectly assumed 2D query inputs, breaking models like DeepSeek-V2 that pass 3D queries. The fix ensures robust handling of both 2D `[num_tokens, hidden]` and 3D `[num_tokens, heads, head_dim]` query tensors by using `query.shape[0]` to extract `num_tokens`. A secondary change improves DeepGEMM platform compatibility by disabling it on unsupported platforms to avoid warnings.

**Technical impact**  
This correction ensures the Attention layer produces the correct output tensor shape `(num_tokens, hidden_size)` regardless of query input dimensionality, preventing downstream failures in FP8 quantization kernels. The DeepGEMM logic update prevents unnecessary warnings and ensures the feature is only enabled on compatible platforms, improving cross-platform consistency.

**Potential risks**  
While the fix addresses the immediate shape issue, there's a risk that other code paths may still assume 2D queries, potentially causing hidden compatibility issues. The DeepGEMM change could inadvertently disable the feature on platforms where it was previously enabled but unsupported, though this aligns with intended behavior.

**Key insights**  
Always validate tensor shape assumptions when handling variable-dimensional inputs, especially in foundational layers like Attention. The fix demonstrates the importance of defensive programming for API robustness. Developers should verify that similar shape assumptions don't exist elsewhere in the codebase, particularly in other attention implementations or quantization layers.

---

## 8. [[BugFix] Support online dense model DP without overhead](https://github.com/vllm-project/vllm/pull/30739)


### Base Information

- **PR Number:** #30739
- **Author:** [njhill](https://github.com/njhill)
- **Merged By:** [youkaichao](https://github.com/youkaichao)
- **Merged time:** 2026-01-02 07:36:38
- **Type:** `SUBSTANTIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/30739/files) (20):**
  - `tests/test_config.py`
  - `tests/v1/engine/test_engine_core_client.py`
  - `vllm/compilation/backends.py`
  - `vllm/compilation/decorators.py`
  - `vllm/config/model.py`
  - `vllm/config/parallel.py`
  - `vllm/config/vllm.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector.py`
  - `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`
  - `vllm/distributed/parallel_state.py`
  - `vllm/engine/arg_utils.py`
  - `vllm/forward_context.py`
  - `vllm/v1/core/sched/scheduler.py`
  - `vllm/v1/engine/coordinator.py`
  - `vllm/v1/engine/core.py`
  - `vllm/v1/engine/core_client.py`
  - `vllm/v1/engine/llm_engine.py`
  - `vllm/v1/engine/utils.py`
  - `vllm/v1/worker/gpu/model_runner.py`
  - `vllm/v1/worker/gpu_worker.py`

### Summary

**What changed and why**  
This PR optimizes data parallel (DP) execution for non-MoE (dense) models by eliminating unnecessary synchronization overhead. Previously, all DP ranks performed redundant all-reduce operations and coordinated dummy forward passes even when running dense models. Now, for non-MoE models, each DP rank operates independently (effectively DP=1), while the DP coordinator is retained only for internal load-balancing stats collection. Wave coordination is disabled for dense models, reducing latency and improving throughput.

**Technical impact**  
The changes introduce a new `data_parallel_index` field to distinguish logical DP rank from process-group ranks, allowing dense models to bypass DP synchronization. The `needs_dp_coordinator` property determines when coordination is required (MoE models or internal/hybrid load balancing). Engine initialization now selects different actor classes (`EngineCoreActor` for dense, `DPMoEEngineCoreActor` for MoE) and adjusts DP configuration dynamically. Offline DP mode is explicitly disallowed for dense models, as it provides no benefit.

**Potential risks**  
- Offline DP for dense models now fails at startup, which may break existing workflows that unintentionally used it.  
- The introduction of `data_parallel_index` could cause confusion if not consistently used instead of `data_parallel_rank` in new code.  
- Edge cases in hybrid load-balancing modes or mixed MoE/dense deployments need careful validation to ensure stats collection and coordination behave correctly.

**Key insights**  
- Dense models no longer incur DP synchronization overhead, yielding measurable performance gains (benchmarks show ~5% higher throughput and reduced latency).  
- Developers must update any custom code that relies on `data_parallel_rank` for non-process-group purposes to use `data_parallel_index`.  
- The change is limited to online/AsyncLLM scenarios; offline DP for dense models is now explicitly unsupported.

---

## 9. [CustomOp: test forward dispatch for grouped_topk](https://github.com/vllm-project/vllm/pull/31530)


### Base Information

- **PR Number:** #31530
- **Author:** [xinyu-intel](https://github.com/xinyu-intel)
- **Merged By:** [yewentao256](https://github.com/yewentao256)
- **Merged time:** 2026-01-02 07:04:01
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31530/files) (1):**
  - `tests/kernels/moe/test_grouped_topk.py`

### Summary

**What changed and why**  
The test now configures a custom VLLM configuration that enables the `grouped_topk` custom operation, then verifies that the forward dispatch mechanism correctly selects the CUDA implementation. This follows up on previous PRs to test forward function dispatch for the custom grouped_topk operation.

**Technical impact**  
The changes ensure that the custom operation dispatch logic works correctly by explicitly setting the compilation configuration to include `grouped_topk`, clearing any cached configurations, and asserting that the CUDA forward method is selected. This validates the integration of custom ops into the model execution pipeline.

**Potential risks**  
If the `set_current_vllm_config` context manager doesn't properly isolate configuration state, it could affect other tests running in parallel. The hardcoded assertion for CUDA forward method assumes CUDA availability, which could cause test failures in non-CUDA environments.

**Key insights**  
The test effectively validates custom op dispatch but should be made more robust by checking platform availability before asserting on CUDA implementation. Consider using a parameterized approach to test different compilation configurations and ensure the context manager properly restores previous state.

---

## 10. [Add multimodal input method in the documentation](https://github.com/vllm-project/vllm/pull/31601)


### Base Information

- **PR Number:** #31601
- **Author:** [labAxiaoming](https://github.com/labAxiaoming)
- **Merged By:** [DarkLight1337](https://github.com/DarkLight1337)
- **Merged time:** 2026-01-02 04:43:30
- **Type:** `TRIVIAL`
- **[Changed Files](https://github.com/vllm-project/vllm/pull/31601/files) (2):**
  - `docs/features/multimodal_inputs.md`
  - `examples/online_serving/openai_chat_completion_client_for_multimodal.py`

### Summary

**What changed and why**  
This PR adds documentation and example code for local image file support in multimodal inputs. The changes demonstrate how to use both file:// URLs and base64-encoded local images with the OpenAI client, complementing existing remote URL examples.

**Technical impact**  
The documentation now provides comprehensive guidance for all three image input methods: remote URLs, local file URLs, and base64 encoding. The example code includes proper error handling with os.path.exists() checks and demonstrates the required --allowed-local-media-path server configuration.

**Potential risks**  
The example uses a placeholder path "/path/to/image.jpg" which could confuse users if not replaced. There's no validation of file formats or size limits. The documentation doesn't explicitly warn about security implications of allowing local file access via --allowed-local-media-path.

**Key insights**  
Developers should update the placeholder path to their actual image files. Consider adding file format validation and size checks in production code. The --allowed-local-media-path server flag is required for local file support but carries security implications that should be documented.

---

